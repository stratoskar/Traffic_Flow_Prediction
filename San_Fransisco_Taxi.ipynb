{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e79ea0",
   "metadata": {},
   "source": [
    "### This notebook process the initial dataset, in order to extract usefull information from it. \n",
    "\n",
    "In this dataset there are trajectories from taxis in San Francisco. All the trajectories provide <b>Latitude</b> and <b>Longitude</b> infromation, as well as <b>Timestamp</b>. All the trajectories have been traced in May 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b028b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-06-01 14:19:48 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# measure execution time\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e615b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.9 s (started: 2023-06-01 14:19:48 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly_express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d0cd6",
   "metadata": {},
   "source": [
    "### Phase 1: Preprocess the dataset\n",
    "In this step, the following commands are executed:\n",
    "- Add Taxi ID on the data\n",
    "- Gather all the data in one txt file\n",
    "- Convert time information to timestamp\n",
    "- Split trajectories, based in the time field\n",
    "- Delete unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d17cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path in which data are stored\n",
    "path = 'C:/Users/SK/Desktop/Diploma_Thesis/Datasets/San-Francisco-Yellow-Cabs/Data'\n",
    "\n",
    "counter = 0 # Taxi ID starts from 0\n",
    "\n",
    "# create an empty dataframe, in which all the data will be saved\n",
    "all_data = pd.DataFrame(columns=['Taxi ID','Latitude','Longitude','Occupied','Date Time'])\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    # read each file in the Data folder\n",
    "    temp = pd.read_csv(path+'/'+filename,names=['Latitude','Longitude','Occupied','Date Time'],sep=' ')\n",
    "    \n",
    "    # assign Taxi ID number to each file\n",
    "    temp.insert(1,'Taxi ID',counter)\n",
    "    \n",
    "    # add the data in this file in the 'all_data' dataframe\n",
    "    all_data = pd.concat([all_data, temp],ignore_index = True)\n",
    "    \n",
    "    counter += 1 # Increase Taxi ID number by 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to txt file\n",
    "all_data.to_csv('C:/Users/SK/Desktop/Diploma_Thesis/Datasets/San-Francisco-Yellow-Cabs/Files/all_data.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2bca2",
   "metadata": {},
   "source": [
    "#### Change datetime field to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Date Time'] = pd.to_datetime(all_data['Date Time'],origin='unix',unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bdd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data of one week\n",
    "all_data = all_data[(all_data['Date Time'] >= \"2008-05-18 00:00:00\") & (all_data['Date Time'] < \"2008-05-25 00:00:00\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c06701",
   "metadata": {},
   "source": [
    "#### Sort the data based in Taxi ID and timestamp information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.sort_values(['Taxi ID','Date Time'])\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f8def",
   "metadata": {},
   "source": [
    "#### Delete the 'Occupied' column\n",
    "This column denotes whether or not the taxi was occupied by a passenger, at the time of GPS recording. So, this information is not usefull for our research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop('Occupied',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a12e2",
   "metadata": {},
   "source": [
    "#### Split the trajectories based in time field and FIle ID\n",
    "\n",
    "Split each trajectory in the same Taxi ID based in the timestamp field.\n",
    "\n",
    "Here, <b>n_sec</b> variable denotes the maximum number of seconds that consecutive GPS traces in the same trajectory should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.insert(1,'Traj ID',-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfdfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Each Taxi ID contains GPS data of one trajectory\n",
    "\n",
    "If the time gap between two GPS points is lower than n_sec seconds, (condition 1)\n",
    "and these GPS points belong to the same Taxi ID  (condition 2)\n",
    "then asign the same Traj ID number. (result)\n",
    "\n",
    "If the time gap between two GPS points is higher than n_sec seconds, (condition 1)\n",
    "and these GPS points belong to the same Taxi ID  (condition 2)\n",
    "then asign different Traj ID number to each of these GPS points. (result)\n",
    "\n",
    "If the GPS points belong to the same Taxi ID  (condition 2)\n",
    "then asign different Traj ID number to each of these GPS points. (result)\n",
    "\n",
    "'''\n",
    "\n",
    "# max number of seconds between GPS records of each traectory\n",
    "n_sec = 90\n",
    "traj_id = 0\n",
    "\n",
    "for i in range(all_data.shape[0] -1):\n",
    "    \n",
    "    if (all_data['Taxi ID'][i+1] == all_data['Taxi ID'][i]): # belong to the same File ID\n",
    "        \n",
    "        if (((all_data['Date Time'][i+1])-(all_data['Date Time'][i])).total_seconds() <= n_sec): # time interval less-equal than n_sec\n",
    "            all_data.at[i,'Traj ID'] = traj_id\n",
    "            all_data.at[i+1,'Traj ID'] = traj_id\n",
    "            \n",
    "        else: # time interval higher than n_sec\n",
    "            all_data.at[i,'Traj ID'] = traj_id\n",
    "            traj_id +=1\n",
    "            all_data.at[i+1,'Traj ID'] = traj_id\n",
    "    \n",
    "    else: # not belong to the same File ID\n",
    "        all_data.at[i,'Traj ID'] = traj_id\n",
    "        traj_id  = 0\n",
    "        all_data.at[i+1,'Traj ID'] = traj_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4434bb8",
   "metadata": {},
   "source": [
    "#### Delete trajectories, which contain only one OSM Way ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a67c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = all_data.loc[:, ['Taxi ID', 'Traj ID']].drop_duplicates(keep=False).index\n",
    "all_data.drop(uniques,axis=0,inplace=True)\n",
    "all_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cd0c4",
   "metadata": {},
   "source": [
    "#### Find min and max date in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac67545",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min date is: \"+str(all_data['Date Time'].min()))\n",
    "print(\"Max date is: \"+str(all_data['Date Time'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75338024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to txt file (with information of splitted trajectories)\n",
    "all_data.to_csv('C:/Users/SK/Desktop/Diploma_Thesis/Datasets/San-Francisco-Yellow-Cabs/splitted_trajectories90.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb51c4",
   "metadata": {},
   "source": [
    "#### Begin Map Matching\n",
    "\n",
    "Map matching done using Valhalla Meili API. Given each trajectory to the API as input, the response contains information of the exact path that each trajectory followed. The paths are in the form of OSM Way IDs. \n",
    "\n",
    "Sources:\n",
    "\n",
    "-  <b>Installation using Docker: </b>https://ikespand.github.io/posts/meili/\n",
    "-  <b>Paper about Valhalla: </b>https://link.springer.com/article/10.1007/s42979-022-01340-5#Tab5\n",
    "-  <b>APIs documentation: </b>https://valhalla.github.io/valhalla/api/map-matching/api-reference/#matched-point-items  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass lat and lot pairs to Valhalla API\n",
    "df_for_meili = all_data[['Latitude','Longitude']]\n",
    "df_for_meili = df_for_meili.rename(columns={\"Latitude\": \"lat\", \"Longitude\": \"lon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13033a66",
   "metadata": {},
   "source": [
    "Create a new dataframe under the name \"visited_segments\", in which information about each trajectory will be contained. The columns of this new dataframe are:\n",
    "-  <b>File ID: </b>The folder that contains information of this trajecotry.\n",
    "-  <b>Traj ID: </b>The ID of the trajectory in this folder.\n",
    "-  <b>OSM Way ID: </b>The way ID number of the edge that trajectory visited.\n",
    "-  <b>Start Time: </b>Expected time that trajecotry enter the specific edge.\n",
    "-  <b>End Time: </b>Expected time that trajecotry left the specific edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a452215",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_segments = pd.DataFrame(columns=['Taxi ID','Traj ID','OSM Way ID','Start Time','End Time'])\n",
    "\n",
    "for taxi_id in all_data['Taxi ID'].unique():\n",
    "    for traj_id in all_data[all_data['Taxi ID'] == taxi_id]['Traj ID'].unique():\n",
    "\n",
    "            # get the batch of data that we send to the request\n",
    "            indexes = all_data[(all_data['Taxi ID']==taxi_id) & (all_data['Traj ID'] == traj_id)].index\n",
    "            \n",
    "            # input to API\n",
    "            passed_data = df_for_meili.iloc[indexes]\n",
    "\n",
    "            # Preparing the request to Valhalla's Meili\n",
    "            meili_coordinates = passed_data.to_json(orient='records')\n",
    "            meili_head = '{\"shape\":'\n",
    "            meili_tail = \"\"\",\"search_radius\": 250, \"sigma_z\": 10, \"beta\": 10,\"shape_match\":\"map_snap\", \"costing\":\"auto\",\n",
    "                            \"filters\":{\"attributes\":[\"edge.way_id\"],\"action\":\"include\"},\n",
    "                            \"format\":\"osrm\"}\"\"\"\n",
    "\n",
    "            # this is the request\n",
    "            meili_request_body = meili_head + meili_coordinates + meili_tail\n",
    "\n",
    "            # the URL of the local valhalla server\n",
    "            url = \"http://localhost:8002/trace_attributes\"\n",
    "\n",
    "            # providing headers to the request\n",
    "            headers = {'Content-type': 'application/json'}\n",
    "\n",
    "            # we need to send the JSON as a string\n",
    "            data = str(meili_request_body)\n",
    "\n",
    "            # sending a request\n",
    "            r = requests.post(url, data=data, headers=headers)\n",
    "\n",
    "            if r.status_code == 200: # response from Valhalla API was successful\n",
    "\n",
    "                # Parsing the JSON response\n",
    "                response_text = json.loads(r.text)\n",
    "\n",
    "                # find the time interval (in sec) that the trajectory needs to be completed [last timestamp - first timestamp]\n",
    "                interval = (all_data.iloc[indexes].iloc[-1]['Date Time'] - all_data.iloc[indexes].iloc[0]['Date Time']).total_seconds()\n",
    "\n",
    "                # compute the expected duration that the moving object is in each edge (duration is equal for each edge that the trajectory visits)\n",
    "                duration  = interval/len(response_text['edges'])\n",
    "\n",
    "                # make a temporary dataframe\n",
    "                temp = pd.DataFrame(columns=['Taxi ID','Traj ID','OSM Way ID','Start Time','End Time'])\n",
    "\n",
    "                # make the final dataframe with the help of a temporary dataframe\n",
    "                for i in range(len(response_text['edges'])):\n",
    "\n",
    "                    # complete the fields of temp dataframe\n",
    "                    temp.at[i,'Taxi ID'] = taxi_id\n",
    "                    temp.at[i,'Traj ID'] = traj_id\n",
    "                    temp.at[i,'OSM Way ID'] = response_text['edges'][i]['way_id']\n",
    "\n",
    "                    if i == 0:\n",
    "                        temp.at[i,'Start Time'] = all_data.iloc[indexes].iloc[0]['Date Time']\n",
    "                    else:\n",
    "                        temp.at[i,'Start Time'] = temp.at[i-1,'End Time']\n",
    "\n",
    "                    temp.at[i,'End Time'] = temp.at[i,'Start Time'] + timedelta(seconds=duration)\n",
    "\n",
    "                # concatenate the two dataframes\n",
    "                visited_segments = pd.concat([visited_segments,temp],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416b0a5",
   "metadata": {},
   "source": [
    "#### Delete trajectories, which contain only one OSM Way ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7da60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = visited_segments.loc[:, ['Taxi ID', 'Traj ID']].drop_duplicates(keep=False).index\n",
    "visited_segments.drop(uniques,axis=0,inplace=True)\n",
    "visited_segments.reset_index(drop=True,inplace=True)\n",
    "visited_segments = visited_segments.sort_values(['Taxi ID','Traj ID','Start Time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de129b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new dataframe to separate txt file\n",
    "visited_segments.to_csv('C:/Users/SK/Desktop/Diploma_Thesis/Datasets/San-Francisco-Yellow-Cabs/Files/visited_segments.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b011d",
   "metadata": {},
   "source": [
    "### Phase 2: Make the time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e87ce9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 6s (started: 2023-06-01 14:20:01 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# read and sort the data. Also, convert timestamps to datetime data type\n",
    "visited_segments = pd.read_csv('C:/Users/SK/Desktop/Diploma_Thesis/Datasets/San-Francisco-Yellow-Cabs/Files/visited_segments.txt')\n",
    "visited_segments['Start Time'] = pd.to_datetime(visited_segments['Start Time'],format='%Y-%m-%d %H:%M:%S.%f')\n",
    "visited_segments['End Time'] = pd.to_datetime(visited_segments['End Time'],format='%Y-%m-%d %H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a470a52",
   "metadata": {},
   "source": [
    "#### Step 1: Create the time information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be55f8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min timestamp value in the dataframe is:  2008-05-18 00:00:00\n",
      "Max timestamp value in the dataframe is:  2008-05-24 23:59:59.000130\n",
      "\n",
      "Total duration in sec in this dataframe is:  604799.00013\n",
      "time: 203 ms (started: 2023-06-01 14:22:07 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# find max and min timestamp in the dataset\n",
    "min_timestamp = visited_segments['Start Time'].min()\n",
    "print(\"Min timestamp value in the dataframe is: \",min_timestamp)\n",
    "\n",
    "max_timestamp = visited_segments['End Time'].max()\n",
    "print(\"Max timestamp value in the dataframe is: \",max_timestamp)\n",
    "\n",
    "\n",
    "# calculate total seconds between those max and min values\n",
    "total_sec = (max_timestamp-min_timestamp).total_seconds()\n",
    "print(\"\\nTotal duration in sec in this dataframe is: \",total_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637d35e",
   "metadata": {},
   "source": [
    "###### Since we have data of one week, we will create time intervals  of half hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3cf1da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-06-01 14:24:29 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# this list contains the time information of our time-series data\n",
    "time_info = []\n",
    "\n",
    "i =0\n",
    "while(True):\n",
    "    if i == 0:\n",
    "        time_info.append(min_timestamp)\n",
    "    else:\n",
    "        time_info.append(time_info[i-1] + timedelta(seconds=1800))\n",
    "    \n",
    "    if (time_info[i]>=max_timestamp):\n",
    "        break\n",
    "    \n",
    "    i+=1 \n",
    "\n",
    "# create pairs of consequtive values of the list time_info\n",
    "time_intervals = list(zip(*[time_info[i:] for i in range(2)])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6988b",
   "metadata": {},
   "source": [
    "#### Step 2: Generate random unique paths of random lengths\n",
    "These paths can be of any length. The number of consecutive edges contained in the path define it's length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f53621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.09 s (started: 2023-06-01 14:24:36 +03:00)\n"
     ]
    }
   ],
   "source": [
    "paths = pd.DataFrame(columns=['Taxi ID','Traj ID','Path','Length']) # empty dataframe, in which all the paths will be saved\n",
    "\n",
    "length_options = [2,3,4,5,6,7,8,9,10,11,12,13,14,15] # define the length options\n",
    "\n",
    "check = [] # this list is used, in order to be sure that paths created are unique\n",
    "\n",
    "indexes = np.arange(visited_segments.shape[0]-max(length_options)+1) # legal indexes in which algorithm can search for paths\n",
    "\n",
    "i = 0\n",
    "n = 1000 # number of paths to be created\n",
    "\n",
    "while (paths.shape[0] < n):\n",
    "    p = [] # this list will save the path created\n",
    "    selected_index = random.choice(indexes) # select randomly an index   \n",
    "    selected_length = random.choice(length_options) # the length of the new path to be created\n",
    "\n",
    "    # check if path is within the same trajectory\n",
    "    if ((visited_segments.at[selected_index,'Taxi ID'] == visited_segments.at[selected_index+(selected_length-1),'Taxi ID']) & \n",
    "        (visited_segments.at[selected_index,'Traj ID'] == visited_segments.at[selected_index+(selected_length-1),'Traj ID'])):\n",
    "        \n",
    "        # iterate to the visited_segments dataframe and extract the consequtive \n",
    "        # [selected_index...index+selected_length] OSM Way IDs\n",
    "        for y in range(selected_length): \n",
    "            p.append(int(visited_segments.at[selected_index+y,'OSM Way ID']))\n",
    "    \n",
    "        # if path is unique, then add it to paths dataframe\n",
    "        if p not in check: \n",
    "            check.append(p)\n",
    "            paths.at[i,'Taxi ID'] = visited_segments.at[selected_index,'Taxi ID']\n",
    "            paths.at[i,'Traj ID'] = visited_segments.at[selected_index,'Traj ID']\n",
    "            paths.at[i,'Path'] = p # add the new path to the dataframe\n",
    "            paths.at[i,'Length'] = selected_length\n",
    "            i += 1\n",
    "        else:\n",
    "            continue      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93a5fd",
   "metadata": {},
   "source": [
    "#### Step 3a: Create the SPQ function\n",
    "\n",
    "This is the main function that will be used for the construction of the time series dataset.\n",
    "The SPQ (Strict Path Query) function, returns all the trajectories [the trajectories are unique (Taxi_ID,Traj_ID) pairs] that passes through given path of edges at a given time interval [time_enter,time_leave].\n",
    "\n",
    "Parameters:\n",
    "- <b>path: </b> The path that the tajectories should EXACTLY follow (edge by edge). This path can be of any length greater or equal to 2 edges.\n",
    "\n",
    "- <b>time_enter: </b>The time, in which the trajectory should enter the first edge of the path given as input.\n",
    "- <b>time_leave: </b>The time, in which the trajectory should leave the last edge of the path given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54686c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-06-01 14:24:40 +03:00)\n"
     ]
    }
   ],
   "source": [
    "def SPQ(path,time_enter,time_leave):\n",
    "    \n",
    "    # length of the path given to the function\n",
    "    path_length = len(path)\n",
    "    \n",
    "    # this list will save temporarily the trajectories that match the SPQ condition\n",
    "    trajectories = []\n",
    "    \n",
    "    # extract only the data that match the time interval given as input\n",
    "    #examined_data = visited_segments[(visited_segments['Start Time'] >= time_enter) & (visited_segments['End Time'] <= time_leave)].reset_index(drop=True)\n",
    "\n",
    "    # find all the indexes, in which the first edge in the path is located\n",
    "    needed_indexes = examined_data[examined_data['OSM Way ID'] == path[0]].index\n",
    "\n",
    "    # iterate through all indexes (note the Taxi_ID and Traj_ID numbers)\n",
    "    for index in needed_indexes:\n",
    "\n",
    "        traj_id = examined_data.at[index,'Traj ID']\n",
    "        taxi_id = examined_data.at[index,'Taxi ID']\n",
    "        inter = 1\n",
    "        \n",
    "        # decide if the row in the next index matches the criteria (same Taxi_ID, same Traj_ID, the path required)\n",
    "        for i in range(1,path_length):\n",
    "            try:\n",
    "                if (not ((examined_data['OSM Way ID'].iloc[index+i] == path[i]) \n",
    "                         & (examined_data['Traj ID'].iloc[index+i] == traj_id) \n",
    "                        & (examined_data['Taxi ID'].iloc[index+i] == taxi_id))):\n",
    "\n",
    "                    break\n",
    "\n",
    "                inter += 1 # if the criteria matches, then increase inter counter by one\n",
    "\n",
    "            except:\n",
    "                pass # index out of bounds exception (do nothing)\n",
    "            \n",
    "        # if the criteria matches as many times as the length of the path, then we found one trajectory\n",
    "        if (path_length == inter):\n",
    "            trajectories.append((taxi_id,traj_id)) # add this trajectory to the trajectories list\n",
    "    \n",
    "    # return the number of trajectories that matches the criteria\n",
    "    return (len([t for t in (set(tuple(i) for i in trajectories))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53f233",
   "metadata": {},
   "source": [
    "#### Step 3b: Create the same SPQ function using postgreSQL\n",
    "\n",
    "In order to do this, we have loaded the visited_segments data into a table called \"visited_segments\". The SPQ function has been rewritten using PLpgsql language. In order to accelerate the execution of this function, we have defined B+ tree indexes on columns of the visited)_segments table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a29d3a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# open conncection with the database\\nimport psycopg2\\n\\n# Connection parameters\\nhostname = \\'localhost\\'\\ndatabase = \\'San Francisco\\'\\nusername = \\'postgres\\'\\npassword = \\'sobadata2\\'\\n\\n# Connect to the PostgreSQL database\\nconnection = psycopg2.connect(\\n    host=hostname,\\n    database=database,\\n    user=username,\\n    password=password\\n)\\n\\ncursor = connection.cursor()\\n\\n# call plpgsql function\\nfunction_call = \"SELECT spq(%s, %s, %s);\"\\n\\n# define the path\\npath = paths.at[9,\\'Path\\'] # Example path array\\n\\n# define the time interval\\ntime_ent = time_intervals[8][0]  # Example timestamp\\ntime_leav = time_intervals[8][1]\\n\\n# call the function with parameters\\ncursor.execute(function_call, (path, time_ent, time_leav))\\n\\n# fetch the results\\nresult = cursor.fetchone()\\n\\nvalue = result[0]\\nprint(\"Result:\", value)\\n\\ncursor.close()\\n\\nSPQ(paths.at[9,\\'Path\\'],time_intervals[8][0],time_intervals[8][1])\\nconnection.close()'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2023-05-30 20:45:50 +03:00)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# open conncection with the database\n",
    "import psycopg2\n",
    "\n",
    "# Connection parameters\n",
    "hostname = 'localhost'\n",
    "database = 'San Francisco'\n",
    "username = 'postgres'\n",
    "password = 'sobadata2'\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "connection = psycopg2.connect(\n",
    "    host=hostname,\n",
    "    database=database,\n",
    "    user=username,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# call plpgsql function\n",
    "function_call = \"SELECT spq(%s, %s, %s);\"\n",
    "\n",
    "# define the path\n",
    "path = paths.at[9,'Path'] # Example path array\n",
    "\n",
    "# define the time interval\n",
    "time_ent = time_intervals[8][0]  # Example timestamp\n",
    "time_leav = time_intervals[8][1]\n",
    "\n",
    "# call the function with parameters\n",
    "cursor.execute(function_call, (path, time_ent, time_leav))\n",
    "\n",
    "# fetch the results\n",
    "result = cursor.fetchone()\n",
    "\n",
    "value = result[0]\n",
    "print(\"Result:\", value)\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "SPQ(paths.at[9,'Path'],time_intervals[8][0],time_intervals[8][1])\n",
    "connection.close()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a8be1",
   "metadata": {},
   "source": [
    "#### Compare the two methods, using plots. The comparation is based in time of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e04d705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.5 s (started: 2023-05-30 20:47:19 +03:00)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Connection parameters\n",
    "hostname = 'localhost'\n",
    "database = 'San Francisco'\n",
    "username = 'postgres'\n",
    "password = 'sobadata2'\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "connection = psycopg2.connect(\n",
    "    host=hostname,\n",
    "    database=database,\n",
    "    user=username,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "# Function to measure the execution time of a script\n",
    "def measure_execution_time(script, input_data):\n",
    "    start_time = time.time()\n",
    "    # Run the script with the input data\n",
    "    exec(script, globals(), {'input_data': input_data})\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return execution_time\n",
    "\n",
    "# Define your scripts\n",
    "script1 = \"\"\"\n",
    "cursor = connection.cursor()\n",
    "function_call = \"SELECT spq(%s, %s, %s);\"\n",
    "cursor.execute(function_call, (input_data[0], input_data[1], input_data[2]))\n",
    "cursor.close()\n",
    "\"\"\"\n",
    "\n",
    "script2 = \"\"\"\n",
    "SPQ(input_data[0], input_data[1], input_data[2])\n",
    "\"\"\"\n",
    "\n",
    "# Define the input data\n",
    "inputs = ['Input 1', 'Input 2', 'Input 3','Input 4','Input 5','Input 6']  # List of input names\n",
    "input_data1 = [paths.at[0,'Path'],str(time_intervals[0][0]),str(time_intervals[0][1])]\n",
    "input_data2 = [paths.at[1,'Path'],str(time_intervals[4][0]),str(time_intervals[4][1])]\n",
    "input_data3 = [paths.at[4,'Path'],str(time_intervals[12][0]),str(time_intervals[12][1])]\n",
    "input_data4 = [paths.at[8,'Path'],str(time_intervals[12][0]),str(time_intervals[12][1])]\n",
    "input_data5 = [paths.at[6,'Path'],str(time_intervals[18][0]),str(time_intervals[18][1])]\n",
    "input_data6 = [paths.at[2,'Path'],str(time_intervals[56][0]),str(time_intervals[56][1])]\n",
    "\n",
    "# Measure the execution times for each input and script\n",
    "execution_times1 = []\n",
    "execution_times2 = []\n",
    "\n",
    "for input_name in inputs:\n",
    "    input_data = globals()[f'input_data{input_name[-1]}']\n",
    "    \n",
    "    execution_time1 = measure_execution_time(script1, input_data)\n",
    "    execution_times1.append(execution_time1)\n",
    "    \n",
    "    execution_time2 = measure_execution_time(script2, input_data)\n",
    "    execution_times2.append(execution_time2)\n",
    "\n",
    "# Create a grouped bar chart to compare the execution times\n",
    "script_names = ['PostgeSQL', 'Python']\n",
    "bar_width = 0.35\n",
    "index = range(len(inputs))\n",
    "\n",
    "plt.bar(index, execution_times1, bar_width, label='PostgeSQL')\n",
    "plt.bar(index, execution_times2, bar_width, label='Python')\n",
    "plt.xticks(index, inputs)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "connection.close() # close connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2806d",
   "metadata": {},
   "source": [
    "#### Step 4: Fill the time series dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2942447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms (started: 2023-06-01 14:24:48 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# create an empty dataframe\n",
    "time_series = pd.DataFrame(columns=time_intervals)\n",
    "time_series.insert(0,'Taxi ID',0)\n",
    "time_series.insert(1,'Traj ID',0)\n",
    "time_series.insert(2,'Path',0)\n",
    "time_series.insert(3,'Length',0)\n",
    "time_series['Path'] = paths['Path']\n",
    "time_series['Length'] = paths['Length']\n",
    "time_series['Taxi ID'] = paths['Taxi ID']\n",
    "time_series['Traj ID'] = paths['Traj ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the dataframe column by column\n",
    "for time in time_intervals:\n",
    "    i = 0\n",
    "    examined_data = visited_segments[(visited_segments['Start Time'] >= time[0]) & (visited_segments['End Time'] <= time[1])].reset_index(drop=True)\n",
    "    for path in paths['Path'].to_list():\n",
    "        time_series.at[i,time] = SPQ(path,time[0],time[1])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dataframe\n",
    "time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5304aa1",
   "metadata": {},
   "source": [
    "### Phase 3: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of paths grouped by length attribute\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(time_series,x='Length')\n",
    "plt.xlabel('Length inputs')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of path lenghts in the dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb80932",
   "metadata": {},
   "source": [
    "#### Plot some time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "trace1 = go.Scatter(\n",
    " x = time_info,\n",
    " y = time_series.iloc[0,4:],\n",
    " mode = 'lines',\n",
    " name = 'Path of Length: '+str(time_series.at[0,'Length'])\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    " x = time_info,\n",
    " y = time_series.iloc[2,4:],\n",
    " mode = 'lines',\n",
    " name = 'Path of Length: '+str(time_series.at[2,'Length'])\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    " x = time_info,\n",
    " y = time_series.iloc[3,4:],\n",
    " mode = 'lines',\n",
    " name = 'Path - Length: '+str(time_series.at[3,'Length'])\n",
    ")\n",
    "\n",
    "trace4 = go.Scatter(\n",
    " x = time_info,\n",
    " y = time_series.iloc[4,4:],\n",
    " mode = 'lines',\n",
    " name = 'Path of Length: '+str(time_series.at[4,'Length'])\n",
    ")\n",
    "\n",
    "trace5 = go.Scatter(\n",
    " x = time_info,\n",
    " y = time_series.iloc[5,4:],\n",
    " mode = 'lines',\n",
    " name = 'Path of Length: '+str(time_series.at[5,'Length'])\n",
    ")\n",
    "\n",
    "trace6 = go.Scatter(\n",
    " x = time_info,\n",
    " y = time_series.iloc[6,4:],\n",
    " mode = 'lines',\n",
    " name = 'Path of Length: '+str(time_series.at[6,'Length'])\n",
    ")\n",
    "\n",
    "trace7 = go.Scatter(\n",
    " x = time_info,\n",
    " y = time_series.iloc[7,4:],\n",
    " mode = 'lines',\n",
    " name = 'Path of Length: '+str(time_series.at[7,'Length'])\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    " title = 'Path Traffic Flow Within One Week',\n",
    " xaxis = {'title' : 'Date'},\n",
    " yaxis = {'title' : 'Traffic Flow'}\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace1,trace2,trace3,trace4,trace5,trace6,trace7], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc2849",
   "metadata": {},
   "source": [
    "### Phase 4: Train models for forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalytics",
   "language": "python",
   "name": "dataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
