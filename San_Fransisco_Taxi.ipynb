{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e79ea0",
   "metadata": {},
   "source": [
    "### This notebook process the initial dataset, in order to extract usefull information from it. \n",
    "\n",
    "In this dataset there are trajectories from taxis in San Francisco. All the trajectories provide <b>Latitude</b> and <b>Longitude</b> infromation, as well as <b>Timestamp</b>. All the trajectories have been traced in May 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b028b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-05-16 11:39:09 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# measure execution time\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e615b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.9 s (started: 2023-05-16 11:39:09 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly_express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d0cd6",
   "metadata": {},
   "source": [
    "### Phase 1: Preprocess the dataset\n",
    "In this step, the following commands are executed:\n",
    "- Add Taxi ID on the data\n",
    "- Gather all the data in one txt file\n",
    "- Convert time information to timestamp\n",
    "- Split trajectories, based in the time field\n",
    "- Delete unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d17cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path in which data are stored\n",
    "path = 'C:/Users/SK/Desktop/Πτυχιακή/Σύνολα Δεδομένων/Sam-Francisco-Yellow-Cabs/Data'\n",
    "\n",
    "counter = 0 # Taxi ID starts from 0\n",
    "\n",
    "# create an empty dataframe, in which all the data will be saved\n",
    "all_data = pd.DataFrame(columns=['Taxi ID','Latitude','Longitude','Occupied','Date Time'])\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    # read each file in the Data folder\n",
    "    temp = pd.read_csv(path+'/'+filename,names=['Latitude','Longitude','Occupied','Date Time'],sep=' ')\n",
    "    \n",
    "    # assign Taxi ID number to each file\n",
    "    temp.insert(1,'Taxi ID',counter)\n",
    "    \n",
    "    # add the data in this file in the 'all_data' dataframe\n",
    "    all_data = pd.concat([all_data, temp],ignore_index = True)\n",
    "    \n",
    "    counter += 1 # Increase Taxi ID number by 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to txt file\n",
    "all_data.to_csv('C:/Users/SK/Desktop/Πτυχιακή/Σύνολα Δεδομένων/Sam-Francisco-Yellow-Cabs/Files/all_data.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_segments = pd.read_csv('C:/Users/SK/Desktop/Πτυχιακή/Σύνολα Δεδομένων/Sam-Francisco-Yellow-Cabs/Files/visited_segments.txt',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2bca2",
   "metadata": {},
   "source": [
    "#### Change datetime field to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Date Time'] = pd.to_datetime(all_data['Date Time'],origin='unix',unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bdd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the data of one week\n",
    "all_data = all_data[(all_data['Date Time'] >= \"2008-05-18 00:00:00\") & (all_data['Date Time'] < \"2008-05-25 00:00:00\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c06701",
   "metadata": {},
   "source": [
    "#### Sort the data based in Taxi ID and timestamp information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.sort_values(['Taxi ID','Date Time'])\n",
    "all_data = all_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f8def",
   "metadata": {},
   "source": [
    "#### Delete the 'Occupied' column\n",
    "This column denotes whether or not the taxi was occupied by a passenger, at the time of GPS recording. So, this information is not usefull for our research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop('Occupied',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a12e2",
   "metadata": {},
   "source": [
    "#### Split the trajectories based in time field and FIle ID\n",
    "\n",
    "Split each trajectory in the same Taxi ID based in the timestamp field.\n",
    "\n",
    "Here, <b>n_sec</b> variable denotes the maximum number of seconds that consecutive GPS traces in the same trajectory should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.insert(1,'Traj ID',-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfdfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Each Taxi ID contains GPS data of one trajectory\n",
    "\n",
    "If the time gap between two GPS points is lower than n_sec seconds, (condition 1)\n",
    "and these GPS points belong to the same Taxi ID  (condition 2)\n",
    "then asign the same Traj ID number. (result)\n",
    "\n",
    "If the time gap between two GPS points is higher than n_sec seconds, (condition 1)\n",
    "and these GPS points belong to the same Taxi ID  (condition 2)\n",
    "then asign different Traj ID number to each of these GPS points. (result)\n",
    "\n",
    "If the GPS points belong to the same Taxi ID  (condition 2)\n",
    "then asign different Traj ID number to each of these GPS points. (result)\n",
    "\n",
    "'''\n",
    "\n",
    "# max number of seconds between GPS records of each traectory\n",
    "n_sec = 90\n",
    "traj_id = 0\n",
    "\n",
    "for i in range(all_data.shape[0] -1):\n",
    "    \n",
    "    if (all_data['Taxi ID'][i+1] == all_data['Taxi ID'][i]): # belong to the same File ID\n",
    "        \n",
    "        if (((all_data['Date Time'][i+1])-(all_data['Date Time'][i])).total_seconds() <= n_sec): # time interval less-equal than n_sec\n",
    "            all_data.at[i,'Traj ID'] = traj_id\n",
    "            all_data.at[i+1,'Traj ID'] = traj_id\n",
    "            \n",
    "        else: # time interval higher than n_sec\n",
    "            all_data.at[i,'Traj ID'] = traj_id\n",
    "            traj_id +=1\n",
    "            all_data.at[i+1,'Traj ID'] = traj_id\n",
    "    \n",
    "    else: # not belong to the same File ID\n",
    "        all_data.at[i,'Traj ID'] = traj_id\n",
    "        traj_id  = 0\n",
    "        all_data.at[i+1,'Traj ID'] = traj_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4434bb8",
   "metadata": {},
   "source": [
    "#### Delete trajectories, which contain only one OSM Way ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a67c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = all_data.loc[:, ['Taxi ID', 'Traj ID']].drop_duplicates(keep=False).index\n",
    "all_data.drop(uniques,axis=0,inplace=True)\n",
    "all_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345cd0c4",
   "metadata": {},
   "source": [
    "#### Find min and max date in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac67545",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min date is: \"+str(all_data['Date Time'].min()))\n",
    "print(\"Max date is: \"+str(all_data['Date Time'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75338024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to txt file (with information of splitted trajectories)\n",
    "all_data.to_csv('C:/Users/SK/Desktop/Πτυχιακή/Σύνολα Δεδομένων/Sam-Francisco-Yellow-Cabs/Files/splitted_trajectories90.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb51c4",
   "metadata": {},
   "source": [
    "#### Begin Map Matching\n",
    "\n",
    "Map matching done using Valhalla Meili API. Given each trajectory to the API as input, the response contains information of the exact path that each trajectory followed. The paths are in the form of OSM Way IDs. \n",
    "\n",
    "Sources:\n",
    "\n",
    "-  <b>Installation using Docker: </b>https://ikespand.github.io/posts/meili/\n",
    "-  <b>Paper about Valhalla: </b>https://link.springer.com/article/10.1007/s42979-022-01340-5#Tab5\n",
    "-  <b>APIs documentation: </b>https://valhalla.github.io/valhalla/api/map-matching/api-reference/#matched-point-items  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass lat and lot pairs to Valhalla API\n",
    "df_for_meili = all_data[['Latitude','Longitude']]\n",
    "df_for_meili = df_for_meili.rename(columns={\"Latitude\": \"lat\", \"Longitude\": \"lon\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13033a66",
   "metadata": {},
   "source": [
    "Create a new dataframe under the name \"visited_segments\", in which information about each trajectory will be contained. The columns of this new dataframe are:\n",
    "-  <b>File ID: </b>The folder that contains information of this trajecotry.\n",
    "-  <b>Traj ID: </b>The ID of the trajectory in this folder.\n",
    "-  <b>OSM Way ID: </b>The way ID number of the edge that trajectory visited.\n",
    "-  <b>Start Time: </b>Expected time that trajecotry enter the specific edge.\n",
    "-  <b>End Time: </b>Expected time that trajecotry left the specific edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a452215",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_segments = pd.DataFrame(columns=['Taxi ID','Traj ID','OSM Way ID','Start Time','End Time'])\n",
    "\n",
    "for taxi_id in all_data['Taxi ID'].unique():\n",
    "    for traj_id in all_data[all_data['Taxi ID'] == taxi_id]['Traj ID'].unique():\n",
    "\n",
    "            # get the batch of data that we send to the request\n",
    "            indexes = all_data[(all_data['Taxi ID']==taxi_id) & (all_data['Traj ID'] == traj_id)].index\n",
    "            \n",
    "            # input to API\n",
    "            passed_data = df_for_meili.iloc[indexes]\n",
    "\n",
    "            # Preparing the request to Valhalla's Meili\n",
    "            meili_coordinates = passed_data.to_json(orient='records')\n",
    "            meili_head = '{\"shape\":'\n",
    "            meili_tail = \"\"\",\"search_radius\": 250, \"sigma_z\": 10, \"beta\": 10,\"shape_match\":\"map_snap\", \"costing\":\"auto\",\n",
    "                            \"filters\":{\"attributes\":[\"edge.way_id\"],\"action\":\"include\"},\n",
    "                            \"format\":\"osrm\"}\"\"\"\n",
    "\n",
    "            # this is the request\n",
    "            meili_request_body = meili_head + meili_coordinates + meili_tail\n",
    "\n",
    "            # the URL of the local valhalla server\n",
    "            url = \"http://localhost:8002/trace_attributes\"\n",
    "\n",
    "            # providing headers to the request\n",
    "            headers = {'Content-type': 'application/json'}\n",
    "\n",
    "            # we need to send the JSON as a string\n",
    "            data = str(meili_request_body)\n",
    "\n",
    "            # sending a request\n",
    "            r = requests.post(url, data=data, headers=headers)\n",
    "\n",
    "            if r.status_code == 200: # response from Valhalla API was successful\n",
    "\n",
    "                # Parsing the JSON response\n",
    "                response_text = json.loads(r.text)\n",
    "\n",
    "                # find the time interval (in sec) that the trajectory needs to be completed [last timestamp - first timestamp]\n",
    "                interval = (all_data.iloc[indexes].iloc[-1]['Date Time'] - all_data.iloc[indexes].iloc[0]['Date Time']).total_seconds()\n",
    "\n",
    "                # compute the expected duration that the moving object is in each edge (duration is equal for each edge that the trajectory visits)\n",
    "                duration  = interval/len(response_text['edges'])\n",
    "\n",
    "                # make a temporary dataframe\n",
    "                temp = pd.DataFrame(columns=['Taxi ID','Traj ID','OSM Way ID','Start Time','End Time'])\n",
    "\n",
    "                # make the final dataframe with the help of a temporary dataframe\n",
    "                for i in range(len(response_text['edges'])):\n",
    "\n",
    "                    # complete the fields of temp dataframe\n",
    "                    temp.at[i,'Taxi ID'] = taxi_id\n",
    "                    temp.at[i,'Traj ID'] = traj_id\n",
    "                    temp.at[i,'OSM Way ID'] = response_text['edges'][i]['way_id']\n",
    "\n",
    "                    if i == 0:\n",
    "                        temp.at[i,'Start Time'] = all_data.iloc[indexes].iloc[0]['Date Time']\n",
    "                    else:\n",
    "                        temp.at[i,'Start Time'] = temp.at[i-1,'End Time']\n",
    "\n",
    "                    temp.at[i,'End Time'] = temp.at[i,'Start Time'] + timedelta(seconds=duration)\n",
    "\n",
    "                # concatenate the two dataframes\n",
    "                visited_segments = pd.concat([visited_segments,temp],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416b0a5",
   "metadata": {},
   "source": [
    "#### Delete trajectories, which contain only one OSM Way ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7da60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = visited_segments.loc[:, ['Taxi ID', 'Traj ID']].drop_duplicates(keep=False).index\n",
    "visited_segments.drop(uniques,axis=0,inplace=True)\n",
    "visited_segments.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de129b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new dataframe to separate txt file\n",
    "visited_segments.to_csv('C:/Users/SK/Desktop/Πτυχιακή/Σύνολα Δεδομένων/Sam-Francisco-Yellow-Cabs/Files/visited_segments.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b011d",
   "metadata": {},
   "source": [
    "### Phase 2: Make the time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e87ce9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 19s (started: 2023-05-16 11:39:29 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# read and sort the data. Also, convert timestamps to datetime data type\n",
    "visited_segments = pd.read_csv('C:/Users/SK/Desktop/Πτυχιακή/Σύνολα Δεδομένων/Sam-Francisco-Yellow-Cabs/Files/visited_segments.txt')\n",
    "visited_segments['Start Time'] = pd.to_datetime(visited_segments['Start Time'],format='%Y-%m-%d %H:%M:%S.%f')\n",
    "visited_segments['End Time'] = pd.to_datetime(visited_segments['End Time'],format='%Y-%m-%d %H:%M:%S.%f')\n",
    "visited_segments = visited_segments.sort_values(['Taxi ID','Traj ID','Start Time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93a5fd",
   "metadata": {},
   "source": [
    "#### Step 1: Create the SPQ function\n",
    "\n",
    "This is the main function that will be used for the construction of the time series dataset.\n",
    "The SPQ (Strict Path Query) function, returns all the trajectories [the trajectories are unique (Taxi_ID,Traj_ID) pairs] that passes through given path of edges at a given time interval [time_enter,time_leave].\n",
    "\n",
    "Parameters:\n",
    "- <b>path: </b> The path that the tajectories should EXACTLY follow (edge by edge). This path can be of any length greater or equal to 2 edges.\n",
    "\n",
    "- <b>time_enter: </b>The time, in which the trajectory should enter the first edge of the path given as input.\n",
    "- <b>time_leave: </b>The time, in which the trajectory should leave the last edge of the path given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54686c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31 ms (started: 2023-05-16 11:42:48 +03:00)\n"
     ]
    }
   ],
   "source": [
    "def SPQ(path,time_enter,time_leave):\n",
    "    \n",
    "    # length of the path given to the function\n",
    "    path_length = len(path)\n",
    "    \n",
    "    # this list will save temporarily the trajectories that match the SPQ condition\n",
    "    trajectories = []\n",
    "    \n",
    "    # this block of code has been added to another place (but it is still part of this function)\n",
    "    # extract only the data that match the time interval given as input\n",
    "    # examined_data = visited_segments[(visited_segments['Start Time'] >= time_enter) &\n",
    "    #                                  (visited_segments['End Time'] <= time_leave)].reset_index(drop=True)\n",
    "\n",
    "    # find all the indexes, in which the first edge in the path is located\n",
    "    needed_indexes = examined_data[examined_data['OSM Way ID'] == path[0]].index\n",
    "\n",
    "    # iterate through all indexes (note the Taxi_ID and Traj_ID numbers)\n",
    "    for index in needed_indexes:\n",
    "\n",
    "        traj_id = examined_data.at[index,'Traj ID']\n",
    "        taxi_id = examined_data.at[index,'Taxi ID']\n",
    "        inter = 1\n",
    "        \n",
    "        # decide if the row in the next index matches the criteria (same Taxi_ID, same Traj_ID, the path required)\n",
    "        for i in range(1,path_length):\n",
    "            try:\n",
    "                if (not ((examined_data['OSM Way ID'].iloc[index+i] == path[i]) \n",
    "                         & (examined_data['Traj ID'].iloc[index+i] == traj_id) \n",
    "                        & (examined_data['Taxi ID'].iloc[index+i] == taxi_id))):\n",
    "\n",
    "                    break\n",
    "\n",
    "                inter += 1 # if the criteria matches, then increase inter counter by one\n",
    "            \n",
    "            # index out of bounds exception\n",
    "            except:\n",
    "                print('-- index out of bounds --')\n",
    "            \n",
    "        # if the criteria matches as many times as the length of the path, then we found one trajectory\n",
    "        # add this trajectory to the trajectories list\n",
    "        if (path_length == inter):\n",
    "            trajectories.append((taxi_id,traj_id))\n",
    "\n",
    "    # return the number of trajectories that matches the criteria\n",
    "    return (len([t for t in (set(tuple(i) for i in trajectories))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee49cd",
   "metadata": {},
   "source": [
    "#### Step 2: Create the time information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9054d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min timestamp value in the dataframe is:  2008-05-18 00:00:00\n",
      "Max timestamp value in the dataframe is:  2008-05-24 23:59:59.000130\n",
      "\n",
      "Total duration in sec in this dataframe is:  604799.00013\n",
      "time: 282 ms (started: 2023-05-16 11:42:48 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# Find max and min timestamp in the dataset\n",
    "min_timestamp = visited_segments['Start Time'].min()\n",
    "print(\"Min timestamp value in the dataframe is: \",min_timestamp)\n",
    "\n",
    "max_timestamp = visited_segments['End Time'].max()\n",
    "print(\"Max timestamp value in the dataframe is: \",max_timestamp)\n",
    "\n",
    "# Calculate total seconds between those max and min values\n",
    "total_sec = (max_timestamp-min_timestamp).total_seconds()\n",
    "print(\"\\nTotal duration in sec in this dataframe is: \",total_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b46a4",
   "metadata": {},
   "source": [
    "###### Since we have data of one week, we will create time intervals  of one hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c693f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 735 ms (started: 2023-05-16 11:42:49 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# This list contains the time information of our time-series data\n",
    "time_info = []\n",
    "\n",
    "i =0\n",
    "while(True):\n",
    "    if i == 0:\n",
    "        time_info.append(min_timestamp)\n",
    "    else:\n",
    "        time_info.append(time_info[i-1] + timedelta(seconds=3600))\n",
    "    \n",
    "    if (time_info[i]>=max_timestamp):\n",
    "        break\n",
    "    \n",
    "    i+=1 \n",
    "\n",
    "# create pairs of consequtive values of the list time_info\n",
    "time_info = list(zip(*[time_info[i:] for i in range(2)])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53f233",
   "metadata": {},
   "source": [
    "#### Step 3: Generate random paths of length 2 and 3\n",
    "These paths can be of any length. The number of consecutive edges contained in the path define it's length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 100 most common appeared edges (OSM Way IDs) in the visited_segments dataframe\n",
    "most_common_edges = pd.DataFrame(visited_segments['OSM Way ID'].value_counts()[0:100].index,columns=['OSM Way ID'])\n",
    "\n",
    "double_paths = [] # list of paths with length 2\n",
    "triple_paths = [] # list of paths with length 3\n",
    "\n",
    "# fill these lists with consequtive paths of length 2 and length 3\n",
    "for i in range (most_common_edges.shape[0]):\n",
    "\n",
    "    index = visited_segments[visited_segments['OSM Way ID']==most_common_edges.at[i,'OSM Way ID']].index\n",
    "    counter = 0\n",
    "    \n",
    "    for item in index:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        sublist2 = []\n",
    "        sublist3 = []\n",
    "        \n",
    "        item1 = visited_segments.at[item,'OSM Way ID']\n",
    "        item2 = visited_segments.at[item+1,'OSM Way ID']\n",
    "        item3 = visited_segments.at[item+2,'OSM Way ID']\n",
    "        \n",
    "        sublist2.append(item1)\n",
    "        sublist2.append(item2)\n",
    "        \n",
    "        sublist3.append(item1)\n",
    "        sublist3.append(item2)\n",
    "        sublist3.append(item3)\n",
    "        \n",
    "        double_paths.append(sublist2)\n",
    "        triple_paths.append(sublist3)\n",
    "        \n",
    "        if counter == 100: # stop the execution\n",
    "            break\n",
    "\n",
    "# create a new dataframe, which contains all the paths\n",
    "d = pd.DataFrame(columns=['Path'])\n",
    "for i in range (len(double_paths)):\n",
    "    d.at[i,'Path'] = double_paths[i]\n",
    "\n",
    "double_paths = d.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "d = pd.DataFrame(columns=['Path'])\n",
    "for i in range (len(double_paths)):\n",
    "    d.at[i,'Path'] = triple_paths[i]\n",
    "\n",
    "triple_paths = d.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "# add all paths in one dataframe\n",
    "paths = pd.concat([double_paths,triple_paths],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2806d",
   "metadata": {},
   "source": [
    "#### Step 4: Fill the time series dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2942447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe\n",
    "time_series = pd.DataFrame(columns=time_info)\n",
    "time_series.insert(0,'Path',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the dataframe column by column\n",
    "for time in time_info:\n",
    "    i = 0\n",
    "    # extract only the data that match the time interval given as input\n",
    "    examined_data = visited_segments[(visited_segments['Start Time'] >= time[0]) & (visited_segments['End Time'] <= time[1])].reset_index(drop=True)\n",
    "    \n",
    "    for path in paths['Path'].to_list():\n",
    "        time_series.at[i,'Path'] = str(path)\n",
    "        time_series.at[i,time] = SPQ(path,time[0],time[1])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataframe\n",
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576cb39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalytics",
   "language": "python",
   "name": "dataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
