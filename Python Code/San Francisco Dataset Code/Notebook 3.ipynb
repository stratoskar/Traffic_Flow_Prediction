{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fafcf1a",
   "metadata": {},
   "source": [
    "In this section of the code, we train four discrete Machine and Deep learning models with the express purpose of forecasting traffic flow along every path encompassed within the time series dataset. Consequently, we conduct a meticulous analysis of the vehicular flux within the city of San Francisco for each pathway, under the guidance and enforcement of the SQP rules.\n",
    "\n",
    "<b>The SPQ rules are fully described here:</b>https://dl.acm.org/doi/abs/10.1145/2666310.2666413"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7eef9",
   "metadata": {},
   "source": [
    "### Phase 4: Preprocess the time series dataset and add some weather data on it\n",
    "In this step, the following commands are executed:\n",
    "- Load time series dataset that created using SPQ function\n",
    "- Preprocess the time series dataset\n",
    "- Read weather data and preprocess them\n",
    "- Aggregate traffic flow data with weather data\n",
    "- Make visualizations of traffic flow per day and per 3-hour interval\n",
    "- Extract features that are helpful to our research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045ff8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.66 s (started: 2023-07-25 19:02:39 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# measure execution time\n",
    "%load_ext autotime\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4511a38",
   "metadata": {},
   "source": [
    "#### Step 1: Load the time series traffic flow dataset that created using SPQ function\n",
    "In this step we are doing the following operations:\n",
    "- Read the data\n",
    "- Change the name of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd03491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 ms (started: 2023-07-25 19:02:41 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "time_series = pd.read_csv('C:/Users/SK/Desktop/Πτυχιακή/Files/time_series.txt')\n",
    "\n",
    "# this list contains the column names\n",
    "columns = [\"Taxi ID\",\"Traj ID\",\"Path\",\"Length\"]\n",
    "\n",
    "# generate the columns of the dataset\n",
    "i =4\n",
    "while(True):\n",
    "    if i == 4:\n",
    "        columns.append(pd.to_datetime('2008-05-18 00:00:00'))\n",
    "    else:\n",
    "        columns.append(columns[i-1] + timedelta(seconds=1800))\n",
    "    \n",
    "    if (columns[i]>=pd.to_datetime('2008-05-24 23:59:59.000130')):\n",
    "        break\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "# delete the last timestamp\n",
    "columns.pop()\n",
    "\n",
    "# assign new column names to our dataframe\n",
    "time_series.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f6be1",
   "metadata": {},
   "source": [
    "#### Step 2: Preprocess the time series dataset\n",
    "In this step we are doing the following operations:\n",
    "- Reshape it to long format using melt function\n",
    "- Encode categorical values\n",
    "- Preprocess the data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0251450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 547 ms (started: 2023-07-25 19:02:42 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# reshape the data from wide format to long format\n",
    "id_cols = ['Taxi ID','Traj ID', 'Path', 'Length']\n",
    "time_cols = time_series.iloc[:,2:].columns\n",
    "time_series = time_series.melt(id_vars=id_cols, value_vars=time_cols, var_name='Time Column', value_name='Traffic Flow')\n",
    "time_series['Time Column'] = pd.to_datetime(time_series['Time Column'])\n",
    "\n",
    "# sort rows by Path and Time\n",
    "time_series.sort_values(by=['Path','Time Column'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b551e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert path column from categorical to numerical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# fit label encoder on the 'Path' column\n",
    "time_series['Path'] = label_encoder.fit_transform(time_series['Path'])\n",
    "\n",
    "# sort the data based in Path column\n",
    "time_series = time_series.sort_values(by=['Path','Time Column'])\n",
    "\n",
    "# print data\n",
    "time_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920001e6",
   "metadata": {},
   "source": [
    "#### Step 3: Read and preprocess the weather data\n",
    "In the weather dataset of San Francisco area, there are many columns that are useless for our research. We just drop them and preprocess the dataset. \n",
    "\n",
    "In this step we are doing the following operations:\n",
    "- Read the weather data\n",
    "- Drop uneccessary informatio n\n",
    "- Encode categorical attributes\n",
    "- Fill Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459ee61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2023-07-25 19:02:47 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# read weather data\n",
    "weather_data = pd.read_csv('C:/Users/SK/desktop/Πτυχιακή/weather_data.csv',sep=',')\n",
    "\n",
    "# drop unecessary columns. \n",
    "# dropped columns can either have NaN values, or not be associated with traffic flow.\n",
    "# columns kept: temperature, humidity, windspeed, sealevelpressure, visibility\n",
    "weather_data = weather_data.drop(['name','feelslike','dew','precip','precipprob','preciptype',\n",
    "                                  'snow','snowdepth','windgust','winddir','cloudcover',\n",
    "                                  'solarradiation','solarenergy','uvindex','severerisk','stations','icon'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663196b8",
   "metadata": {},
   "source": [
    "There is one column that contains categorical vales. Use one-hot encoding to represent them as numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8df650c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partially cloudy    435\n",
       "Clear               166\n",
       "Overcast            142\n",
       "Rain, Overcast        1\n",
       "Name: conditions, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2023-07-25 19:02:49 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# view the values of \"conditions\" column\n",
    "weather_data['conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2498273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-07-25 19:02:50 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# change the only 'Rain, Overcast' value to 'Overcast'\n",
    "weather_data.loc[weather_data['conditions'] == 'Rain, Overcast', 'conditions'] = 'Overcast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a34dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Partially cloudy    435\n",
       "Clear               166\n",
       "Overcast            143\n",
       "Name: conditions, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-07-25 19:02:50 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# view again the values of \"conditions\" column\n",
    "weather_data['conditions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665f32ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-07-25 19:02:51 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the  \"conditions\" column\n",
    "conditions = pd.get_dummies(weather_data['conditions'], prefix='conditions')\n",
    "weather_data.drop('conditions',axis=1,inplace=True)\n",
    "\n",
    "# concatenate the one-hot encoded columns to the original DataFrame\n",
    "weather_data = pd.concat([weather_data, conditions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38da7c",
   "metadata": {},
   "source": [
    "Now, there are not categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f969fa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 744 entries, 0 to 743\n",
      "Data columns (total 9 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   datetime                     744 non-null    object \n",
      " 1   temp                         744 non-null    float64\n",
      " 2   humidity                     744 non-null    float64\n",
      " 3   windspeed                    744 non-null    float64\n",
      " 4   sealevelpressure             744 non-null    float64\n",
      " 5   visibility                   744 non-null    float64\n",
      " 6   conditions_Clear             744 non-null    uint8  \n",
      " 7   conditions_Overcast          744 non-null    uint8  \n",
      " 8   conditions_Partially cloudy  744 non-null    uint8  \n",
      "dtypes: float64(5), object(1), uint8(3)\n",
      "memory usage: 37.2+ KB\n",
      "time: 16 ms (started: 2023-07-25 19:02:51 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# view for null values\n",
    "weather_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34fcb4",
   "metadata": {},
   "source": [
    "There are not null values in the weather dataset too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f11ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2023-07-25 19:02:52 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# covnert datetime column to appropriate format\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['datetime'])\n",
    "\n",
    "# drop rows that are not in within the interval [2008-05-18,2008-05-24]\n",
    "start_date = pd.to_datetime('2008-05-18')\n",
    "end_date = pd.to_datetime('2008-05-25')\n",
    "\n",
    "weather_data = weather_data[(weather_data['datetime'] >= start_date) & (weather_data['datetime'] <= end_date)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a988f99",
   "metadata": {},
   "source": [
    "#### Step 4: Connect the two dataframes to one\n",
    "Use time information to correctly connect the weather data and traffic flow data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc1c574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 ms (started: 2023-07-25 19:02:53 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# connect two dataframes, time-series traffic flow and weather data\n",
    "time_series.set_index('Time Column', inplace=True)\n",
    "weather_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# resample the 'weather_data' dataset to 30-minute intervals and forward-fill the missing values\n",
    "weather_data = weather_data.resample('30T').ffill()\n",
    "\n",
    "# merge the time_series and weather_data_resampled datasets based on their indexes\n",
    "time_series_weather = time_series.merge(weather_data, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4827f6",
   "metadata": {},
   "source": [
    "#### Step 5: Feature Creation From Time Information\n",
    "In this step we define some features based on time series timestamps.\n",
    "- Day, dayfweek, hour and minute features derive directly from the timestamp column of the dataset.\n",
    "- Hour_sin and Hour_cos columns transform the hour values into a cyclic representation, where the values range from -1 to 1. This transformation is used in time series analysis to capture the periodic patterns or cyclical nature of time-related data.\n",
    "- Use also the 3_hour_interval to describe, in which interval of the day data refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2f883b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms (started: 2023-07-25 19:02:58 +03:00)\n"
     ]
    }
   ],
   "source": [
    "time_series_weather = time_series_weather.reset_index()\n",
    "time_series_weather.rename(columns={'index': 'Time Column'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "422f8f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 266 ms (started: 2023-07-25 19:02:59 +03:00)\n"
     ]
    }
   ],
   "source": [
    "### extract timestamp information to different columns ###\n",
    "\n",
    "time_series_weather['hour'] = time_series_weather['Time Column'].dt.hour\n",
    "time_series_weather['dayofweek'] = time_series_weather['Time Column'].dt.dayofweek\n",
    "time_series_weather['day'] = time_series_weather['Time Column'].dt.day\n",
    "time_series_weather['minute'] = time_series_weather['Time Column'].dt.minute\n",
    "\n",
    "### circular Encoding for cyclic time features ###\n",
    "\n",
    "time_series_weather['hour_sin'] = np.sin(2 * np.pi * time_series_weather['hour'] / 24)\n",
    "time_series_weather['hour_cos'] = np.cos(2 * np.pi * time_series_weather['hour'] / 24)\n",
    "\n",
    "time_series_weather['day_of_week_sin'] = np.sin(2 * np.pi * time_series_weather['dayofweek'] / 7)\n",
    "time_series_weather['day_of_week_cos'] = np.cos(2 * np.pi * time_series_weather['dayofweek'] / 7)\n",
    "\n",
    "time_series_weather['day_sin'] = np.sin(2 * np.pi * time_series_weather['day'] / 31)\n",
    "time_series_weather['day_cos'] = np.cos(2 * np.pi * time_series_weather['day'] / 31)\n",
    "\n",
    "time_series_weather['minute_sin'] = np.sin(2 * np.pi * time_series_weather['minute'] / 60)\n",
    "time_series_weather['minute_cos'] = np.cos(2 * np.pi * time_series_weather['minute'] / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0701e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-07-25 19:03:00 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# define a custom function to determine the three-hour interval, based in timestamp hour information\n",
    "def get_3hour_interval(hour):\n",
    "    if hour in [0, 1, 2]:\n",
    "        return 1\n",
    "    elif hour in [3, 4, 5]:\n",
    "        return 2\n",
    "    elif hour in [6, 7, 8]:\n",
    "        return 3\n",
    "    elif hour in [9, 10, 11]:\n",
    "        return 4\n",
    "    elif hour in [12, 13, 14]:\n",
    "        return 5\n",
    "    elif hour in [15, 16, 17]:\n",
    "        return 6\n",
    "    elif hour in [18, 19, 20]:\n",
    "        return 7\n",
    "    elif hour in [21, 22, 23]:\n",
    "        return 8\n",
    "    else:\n",
    "        return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed3e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 312 ms (started: 2023-07-25 19:03:00 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# apply the custom function on the data to create the '3hour_interval' column\n",
    "time_series_weather['3hour_interval'] = time_series_weather['hour'].apply(get_3hour_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "132838ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time Column</th>\n",
       "      <th>Taxi ID</th>\n",
       "      <th>Traj ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Length</th>\n",
       "      <th>Traffic Flow</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>sealevelpressure</th>\n",
       "      <th>...</th>\n",
       "      <th>minute</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_of_week_sin</th>\n",
       "      <th>day_of_week_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>minute_sin</th>\n",
       "      <th>minute_cos</th>\n",
       "      <th>3hour_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-05-18 00:00:00</td>\n",
       "      <td>97</td>\n",
       "      <td>297</td>\n",
       "      <td>[100400941, 100400941, 100400941, 100400941, 1...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>12.4</td>\n",
       "      <td>87.29</td>\n",
       "      <td>16.1</td>\n",
       "      <td>1017.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-05-18 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>[100400941, 100400941, 144346316]</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12.4</td>\n",
       "      <td>87.29</td>\n",
       "      <td>16.1</td>\n",
       "      <td>1017.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-05-18 00:00:00</td>\n",
       "      <td>247</td>\n",
       "      <td>370</td>\n",
       "      <td>[100400946, 91181759, 91181759, 91181759, 9118...</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>87.29</td>\n",
       "      <td>16.1</td>\n",
       "      <td>1017.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-05-18 00:00:00</td>\n",
       "      <td>327</td>\n",
       "      <td>78</td>\n",
       "      <td>[1011710196, 997142479]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>12.4</td>\n",
       "      <td>87.29</td>\n",
       "      <td>16.1</td>\n",
       "      <td>1017.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-05-18 00:00:00</td>\n",
       "      <td>363</td>\n",
       "      <td>56</td>\n",
       "      <td>[1020512097, 517260065, 517260065, 416877246, ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>87.29</td>\n",
       "      <td>16.1</td>\n",
       "      <td>1017.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.485302</td>\n",
       "      <td>-0.874347</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335995</th>\n",
       "      <td>2008-05-24 23:30:00</td>\n",
       "      <td>403</td>\n",
       "      <td>113</td>\n",
       "      <td>[985061724, 985061724, 985061724, 985061719, 9...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12.4</td>\n",
       "      <td>72.31</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.988468</td>\n",
       "      <td>0.151428</td>\n",
       "      <td>5.665539e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335996</th>\n",
       "      <td>2008-05-24 23:30:00</td>\n",
       "      <td>477</td>\n",
       "      <td>44</td>\n",
       "      <td>[99231463, 99231463, 680402688, 680402688, 120...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>72.31</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.988468</td>\n",
       "      <td>0.151428</td>\n",
       "      <td>5.665539e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335997</th>\n",
       "      <td>2008-05-24 23:30:00</td>\n",
       "      <td>135</td>\n",
       "      <td>125</td>\n",
       "      <td>[996695182, 996695182]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12.4</td>\n",
       "      <td>72.31</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.988468</td>\n",
       "      <td>0.151428</td>\n",
       "      <td>5.665539e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335998</th>\n",
       "      <td>2008-05-24 23:30:00</td>\n",
       "      <td>192</td>\n",
       "      <td>164</td>\n",
       "      <td>[997349781, 99278921, 99278921, 923712624, 923...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>72.31</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.988468</td>\n",
       "      <td>0.151428</td>\n",
       "      <td>5.665539e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335999</th>\n",
       "      <td>2008-05-24 23:30:00</td>\n",
       "      <td>468</td>\n",
       "      <td>131</td>\n",
       "      <td>[997431147, 399156533, 399156533, 997431145]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12.4</td>\n",
       "      <td>72.31</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.988468</td>\n",
       "      <td>0.151428</td>\n",
       "      <td>5.665539e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336000 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Time Column  Taxi ID  Traj ID  \\\n",
       "0      2008-05-18 00:00:00       97      297   \n",
       "1      2008-05-18 00:00:00        2       83   \n",
       "2      2008-05-18 00:00:00      247      370   \n",
       "3      2008-05-18 00:00:00      327       78   \n",
       "4      2008-05-18 00:00:00      363       56   \n",
       "...                    ...      ...      ...   \n",
       "335995 2008-05-24 23:30:00      403      113   \n",
       "335996 2008-05-24 23:30:00      477       44   \n",
       "335997 2008-05-24 23:30:00      135      125   \n",
       "335998 2008-05-24 23:30:00      192      164   \n",
       "335999 2008-05-24 23:30:00      468      131   \n",
       "\n",
       "                                                     Path  Length  \\\n",
       "0       [100400941, 100400941, 100400941, 100400941, 1...       8   \n",
       "1                       [100400941, 100400941, 144346316]       3   \n",
       "2       [100400946, 91181759, 91181759, 91181759, 9118...      10   \n",
       "3                                 [1011710196, 997142479]       2   \n",
       "4       [1020512097, 517260065, 517260065, 416877246, ...      11   \n",
       "...                                                   ...     ...   \n",
       "335995  [985061724, 985061724, 985061724, 985061719, 9...       6   \n",
       "335996  [99231463, 99231463, 680402688, 680402688, 120...       9   \n",
       "335997                             [996695182, 996695182]       2   \n",
       "335998  [997349781, 99278921, 99278921, 923712624, 923...      12   \n",
       "335999       [997431147, 399156533, 399156533, 997431145]       4   \n",
       "\n",
       "        Traffic Flow  temp  humidity  windspeed  sealevelpressure  ...  \\\n",
       "0                  4  12.4     87.29       16.1            1017.7  ...   \n",
       "1                  4  12.4     87.29       16.1            1017.7  ...   \n",
       "2                  7  12.4     87.29       16.1            1017.7  ...   \n",
       "3                  5  12.4     87.29       16.1            1017.7  ...   \n",
       "4                  0  12.4     87.29       16.1            1017.7  ...   \n",
       "...              ...   ...       ...        ...               ...  ...   \n",
       "335995             2  12.4     72.31       10.5            1015.0  ...   \n",
       "335996             0  12.4     72.31       10.5            1015.0  ...   \n",
       "335997             3  12.4     72.31       10.5            1015.0  ...   \n",
       "335998             0  12.4     72.31       10.5            1015.0  ...   \n",
       "335999             1  12.4     72.31       10.5            1015.0  ...   \n",
       "\n",
       "        minute  hour_sin  hour_cos  day_of_week_sin  day_of_week_cos  \\\n",
       "0            0  0.000000  1.000000        -0.781831         0.623490   \n",
       "1            0  0.000000  1.000000        -0.781831         0.623490   \n",
       "2            0  0.000000  1.000000        -0.781831         0.623490   \n",
       "3            0  0.000000  1.000000        -0.781831         0.623490   \n",
       "4            0  0.000000  1.000000        -0.781831         0.623490   \n",
       "...        ...       ...       ...              ...              ...   \n",
       "335995      30 -0.258819  0.965926        -0.974928        -0.222521   \n",
       "335996      30 -0.258819  0.965926        -0.974928        -0.222521   \n",
       "335997      30 -0.258819  0.965926        -0.974928        -0.222521   \n",
       "335998      30 -0.258819  0.965926        -0.974928        -0.222521   \n",
       "335999      30 -0.258819  0.965926        -0.974928        -0.222521   \n",
       "\n",
       "         day_sin   day_cos    minute_sin  minute_cos  3hour_interval  \n",
       "0      -0.485302 -0.874347  0.000000e+00         1.0               1  \n",
       "1      -0.485302 -0.874347  0.000000e+00         1.0               1  \n",
       "2      -0.485302 -0.874347  0.000000e+00         1.0               1  \n",
       "3      -0.485302 -0.874347  0.000000e+00         1.0               1  \n",
       "4      -0.485302 -0.874347  0.000000e+00         1.0               1  \n",
       "...          ...       ...           ...         ...             ...  \n",
       "335995 -0.988468  0.151428  5.665539e-16        -1.0               8  \n",
       "335996 -0.988468  0.151428  5.665539e-16        -1.0               8  \n",
       "335997 -0.988468  0.151428  5.665539e-16        -1.0               8  \n",
       "335998 -0.988468  0.151428  5.665539e-16        -1.0               8  \n",
       "335999 -0.988468  0.151428  5.665539e-16        -1.0               8  \n",
       "\n",
       "[336000 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 219 ms (started: 2023-07-25 19:03:01 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# print dataset\n",
    "time_series_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd99501",
   "metadata": {},
   "source": [
    "#### Step 6: Make Visualizations\n",
    "\n",
    "In this step, we are doing the following operations:\n",
    "- View total traffic flow (sum of traffic flow of every path in the dataset) per day. In this way we understand patterns that traffic flow has.\n",
    "\n",
    "- View total traffic flow per day and 3-hour interval of the same day, to understand in which time of each day Traffic Flow is on peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cddea8",
   "metadata": {},
   "source": [
    "##### Visualise the Traffic Flow in every path of the dataset per day\n",
    "Use the sum of traffic flow of every path at each timestep to generate a total overview about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a002c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by timestamp (index) and calculate the sum of the traffic flow\n",
    "grouped_df = time_series_weather['Traffic Flow'].groupby(time_series_weather['Time Column']).sum()\n",
    "\n",
    "# add results to dataframe\n",
    "grouped_df = pd.DataFrame(grouped_df,index=grouped_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional time information in the dataset\n",
    "grouped_df['hour'] = grouped_df.index.hour\n",
    "grouped_df['3hour_interval'] = grouped_df['hour'].apply(get_3hour_interval)\n",
    "grouped_df['dayofweek'] = grouped_df.index.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb82aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom dark color palette\n",
    "dark_palette = sns.color_palette('dark', n_colors=8)\n",
    "\n",
    "# create a plot to view the results\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(grouped_df,x=grouped_df.index,y='Traffic Flow',hue='dayofweek',marker='o', palette=dark_palette,linewidth=2.5)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sum of Traffic Flow')\n",
    "plt.title('Sum of Traffic Flow in Every Path of the Dataset Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4eca1b",
   "metadata": {},
   "source": [
    "Based on graph above, we see that for each day, there is a seasonality pattern. In the noon hours Traffic Flow is lower than in the mornnings or in the evenings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036d0ff",
   "metadata": {},
   "source": [
    "##### Visualize traffic flow per day and per 3hour time inteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671cf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic flow on 2008-05-18\n",
    "pl = grouped_df[(grouped_df.index >= '2008-05-18 00:00:00') & (grouped_df.index < '2008-05-19 00:00:00')]\n",
    "\n",
    "# Create a custom dark color palette\n",
    "dark_palette = sns.color_palette('dark', n_colors=8)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(pl,x='Time Column',y='Traffic Flow',hue='3hour_interval',marker='o', palette=dark_palette,linewidth=2.5)\n",
    "plt.title(\"Traffic Flow on 2008-05-18\")\n",
    "plt.xlabel(\"Time information every 30 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic flow on 2008-05-19\n",
    "pl = grouped_df[(grouped_df.index >= '2008-05-19 00:00:00') & (grouped_df.index < '2008-05-20 00:00:00')]\n",
    "\n",
    "# Create a custom dark color palette\n",
    "dark_palette = sns.color_palette('dark', n_colors=8)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(pl,x='Time Column',y='Traffic Flow',hue='3hour_interval',marker='o', palette=dark_palette,linewidth=2.5)\n",
    "plt.title(\"Traffic Flow on 2008-05-19\")\n",
    "plt.xlabel(\"Time information every 30 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic flow on 2008-05-20\n",
    "pl = grouped_df[(grouped_df.index >= '2008-05-20 00:00:00') & (grouped_df.index < '2008-05-21 00:00:00')]\n",
    "\n",
    "# Create a custom dark color palette\n",
    "dark_palette = sns.color_palette('dark', n_colors=8)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(pl,x='Time Column',y='Traffic Flow',hue='3hour_interval',marker='o', palette=dark_palette,linewidth=2.5)\n",
    "plt.title(\"Traffic Flow on 2008-05-20\")\n",
    "plt.xlabel(\"Time information every 30 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic flow on 2008-05-21\n",
    "pl = grouped_df[(grouped_df.index >= '2008-05-21 00:00:00') & (grouped_df.index < '2008-05-22 00:00:00')]\n",
    "\n",
    "# Create a custom dark color palette\n",
    "dark_palette = sns.color_palette('dark', n_colors=8)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(pl,x='Time Column',y='Traffic Flow',hue='3hour_interval',marker='o', palette=dark_palette,linewidth=2.5)\n",
    "plt.title(\"Traffic Flow on 2008-05-21\")\n",
    "plt.xlabel(\"Time information every 30 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ded2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic flow on 2008-05-22\n",
    "pl = grouped_df[(grouped_df.index >= '2008-05-22 00:00:00') & (grouped_df.index < '2008-05-23 00:00:00')]\n",
    "\n",
    "# Create a custom dark color palette\n",
    "dark_palette = sns.color_palette('dark', n_colors=8)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(pl,x='Time Column',y='Traffic Flow',hue='3hour_interval',marker='o', palette=dark_palette,linewidth=2.5)\n",
    "plt.title(\"Traffic Flow on 2008-05-22\")\n",
    "plt.xlabel(\"Time information every 30 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e993193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic flow on 2008-05-23\n",
    "pl = grouped_df[(grouped_df.index >= '2008-05-23 00:00:00') & (grouped_df.index < '2008-05-24 00:00:00')]\n",
    "\n",
    "# Create a custom dark color palette\n",
    "dark_palette = sns.color_palette('dark', n_colors=8)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(pl,x='Time Column',y='Traffic Flow',hue='3hour_interval',marker='o', palette=dark_palette,linewidth=2.5)\n",
    "plt.title(\"Traffic Flow on 2008-05-23\")\n",
    "plt.xlabel(\"Time information every 30 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06828bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic flow on 2008-05-24\n",
    "pl = grouped_df[(grouped_df.index >= '2008-05-24 00:00:00') & (grouped_df.index < '2008-05-25 00:00:00')]\n",
    "\n",
    "# Create a custom dark color palette\n",
    "dark_palette = sns.color_palette('dark', n_colors=8)\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(pl,x='Time Column',y='Traffic Flow',hue='3hour_interval',marker='o', palette=dark_palette,linewidth=2.5)\n",
    "plt.title(\"Traffic Flow on 2008-05-24\")\n",
    "plt.xlabel(\"Time information every 30 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee100a",
   "metadata": {},
   "source": [
    "##### Find highly correlated data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(time_series_weather.corr(), annot=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611afdb",
   "metadata": {},
   "source": [
    "Based on the above heatmap, we see that there are some features that are highly correlated. For example,\n",
    "temperature is correlated wiith humidity, windspeed, the hour of day, 3hour_interval and circular encoding of hour feature. Based on this heatmap above, we can choose the features that will be used in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e147a",
   "metadata": {},
   "source": [
    "### Phase 5: Build machine learning models for traffic flow forecasting\n",
    "In this step, the following commands are executed:\n",
    "- Train and Test split\n",
    "- Find the best lookback timesteps for our XGBoost model\n",
    "- Perform Grid Search CV to find the optimum parameters for the XGBoost model\n",
    "- Train XGboost model (1)\n",
    "- Train an LSTM model (2)\n",
    "- Train a random forest model (3)\n",
    "- Perform Grid Search CV to find the optimum parameters for the random forest model\n",
    "- Train Encodel-Decoder model (4)\n",
    "- Visualise the losses for each model\n",
    "- Test on testing set each model\n",
    "- Add in the same dataframe the scores of each model, in order to compare them\n",
    "- Make visualizations on actual vs predicted traffic flow data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7951058",
   "metadata": {},
   "source": [
    "#### Step 1: Train - Test split\n",
    "In this step, we split the data into two datasets train and test.\n",
    "The time inteval of all observations in this dataset are within [18-05-2008, 24-05-2008].\n",
    "- Train dataset contains all the data for each path till 2008-05-23 inclusive.\n",
    "- The rest of the data (the most resent ones) are in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8291689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the time series data\n",
    "time_series_weather.sort_values(by=['Path','Time Column'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train data\n",
    "train = time_series_weather.loc[time_series_weather['Time Column'] < '2008-05-24']\n",
    "\n",
    "# define test data\n",
    "test = time_series_weather.loc[time_series_weather['Time Column'] >= '2008-05-24']\n",
    "\n",
    "# print the results\n",
    "print(\"Train set shape: \",train.shape)\n",
    "print(\"Test set shape: \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine train and test sets based on correlated features (drop highly correlated features)\n",
    "train = train.drop(['Time Column','hour', 'hour_sin','day_of_week_cos', 'dayofweek', 'day', 'minute'],axis=1)\n",
    "test = test.drop(['Time Column','hour', 'hour_sin','day_of_week_cos', 'dayofweek', 'day', 'minute'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247bbcc7",
   "metadata": {},
   "source": [
    "#### Step 2: Find the best lookback timesteps for our XGBoost model\n",
    "In this step, we define the optimum lookback timestep number for our sliding window. This optimum number will be used by the model for forecasting. Also, since XGBoost is the best model based in RMSE score, we perform this operation only for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fdf4b",
   "metadata": {},
   "source": [
    "##### Step 2a: Create empty dataframe to store the scores per lookback timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best lookback timesteps to perform analysis. Save those scores in a separate dataframe\n",
    "lookback_scores = pd.DataFrame(columns=['#Timesteps','XGBoost Score'])\n",
    "lookback_scores['#Timesteps'] = 0\n",
    "lookback_scores['XGBoost Score'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d0187",
   "metadata": {},
   "source": [
    "##### Step 2b: Create function that converts the data into a supervised problem\n",
    "Consider for each path n_in timesteps of traffic flow in the past, in order to predict the n_out traffic flow timesteps in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a227f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "# create functio for generating (X, y) pairs\n",
    "def series_to_supervised(data, n_in=1, n_out=1):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('Traffic Flow %d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('Traffic Flow %d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('Traffic Flow %d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the training data into a supervised dataset. Create X,y pairs of the traffic column information.\n",
    "Then, concatenate the rest of the culumns. \n",
    "\"\"\"\n",
    "def generate_X_y_pairs(data,n_in=1,n_out=1):\n",
    "    new_data = pd.DataFrame()\n",
    "    for item in data['Path'].unique():\n",
    "        \n",
    "        # call function only for traffic flow columns\n",
    "        supervised_traffic_flow = series_to_supervised(pd.DataFrame(data[data['Path'] == item]['Traffic Flow']), n_in, n_out)\n",
    "        \n",
    "        # the rest of the columns are concatenated as they were\n",
    "        supervised_traffic_flow = pd.concat([supervised_traffic_flow, time_series_weather[time_series_weather['Path'] == item]], axis=1)\n",
    "        \n",
    "        new_data = pd.concat([new_data,supervised_traffic_flow])\n",
    "\n",
    "    new_data.dropna(inplace=True)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6ed90",
   "metadata": {},
   "source": [
    "##### Step 2c: Create a loop. In each loop, we are doing the following:\n",
    "- 0. Convert train and test sets into supervised problem with lookback timesteps.\n",
    "- 1. Define features and labels.\n",
    "- 2. Train the XGboost model with defult parameters\n",
    "- 3. Predict on the test set\n",
    "- 4. Save RMSE in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58283d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lookback in [2,3,4,5,6]: # different lookback sizes\n",
    "    \n",
    "    # convert train and test data to supervised\n",
    "    train_s = generate_X_y_pairs(train,lookback)\n",
    "    test_s = generate_X_y_pairs(test,lookback)\n",
    "    \n",
    "    print(\"Lookback = \"+str(lookback)+\" - To supervised: Completed!\")\n",
    "\n",
    "    # add rolling mean information\n",
    "    train_s['rolling_mean'] = 0\n",
    "    test_s['rolling_mean'] = 0\n",
    "    \n",
    "    train_s['rolling_mean'] = train_s.iloc[:,0:lookback].mean(axis=1)\n",
    "    test_s['rolling_mean'] = test_s.iloc[:,0:lookback].mean(axis=1)\n",
    "    \n",
    "    # define features\n",
    "    features_s = train_s.drop(['Time Column','Traffic Flow','Traffic Flow 1(t)'],axis=1).columns\n",
    "    \n",
    "    # define label\n",
    "    labels_s = ['Traffic Flow 1(t)']\n",
    "    \n",
    "    # create X_train and y_train datasets\n",
    "    X_train_s = train_s[features_s]\n",
    "    y_train_s = train_s[labels_s]\n",
    "    \n",
    "    # create X_test and y_test datasets\n",
    "    X_test_s = test_s[features_s]\n",
    "    y_test_s = test_s[labels_s]\n",
    "    \n",
    "    print(\"Lookback = \"+str(lookback)+\" - Train and Test sets: Completed!\")\n",
    "\n",
    "    # define XGBoost regression model\n",
    "    reg = xgb.XGBRegressor(objective='reg:squarederror',n_estimators=1000,early_stopping_rounds = 5)\n",
    "    \n",
    "    # train the XGBoost model with feature weights, use validation set on test data)\n",
    "    reg.fit(X_train_s, y_train_s, eval_set=[(X_train_s, y_train_s), (X_test_s,y_test_s)],verbose=False)\n",
    "    \n",
    "    print(\"Lookback = \"+str(lookback)+\" - Trainning: Completed!\")\n",
    "    \n",
    "    # make predictions\n",
    "    y_pred_s = reg.predict(X_test_s)\n",
    "    \n",
    "    # calculate RMSE score\n",
    "    rmse = mean_squared_error(y_test_s, y_pred_s, squared=False)\n",
    "   \n",
    "    # create a new row as a dictionary\n",
    "    new_row = {'#Timesteps': lookback, 'XGBoost Score': rmse}\n",
    "\n",
    "    # append the new row to the DataFrame\n",
    "    lookback_scores = lookback_scores.append(new_row, ignore_index=True)\n",
    "    \n",
    "    print(\"Lookback = \"+str(lookback)+\" - Saved Results: Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced106fc",
   "metadata": {},
   "source": [
    "#####  Step 2d: Print RMSE scores over lookback timesteps into a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0820315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "plt.plot(lookback_scores['#Timesteps'], lookback_scores['XGBoost Score'], linestyle='-', marker='o', color='b')\n",
    "\n",
    "# set plot title and axis labels\n",
    "plt.title('Lookback timesteps over RMSE score')\n",
    "plt.xlabel('Lookback Timesteps of column Traffic Flow')\n",
    "plt.ylabel('RMSE score after XGBoost testing')\n",
    "\n",
    "# add gridlines\n",
    "plt.grid(True)\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the timesteps corresponding to the minimum RMSE score\n",
    "optimum_lookback = int(lookback_scores['#Timesteps'].loc[lookback_scores['XGBoost Score'].idxmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c77587",
   "metadata": {},
   "source": [
    "#### Step 3: Perform Grid Search CV to find the optimum parameters for the XGBoost model\n",
    "This step is essential to tune correctly the XGboost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56515cfa",
   "metadata": {},
   "source": [
    "##### Step 3a: Split train and test into supervised problem, based on optimum lookback timesteps that found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a55a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train and test data to supervised problem based in the optimum lookback step we defined above.\n",
    "train = generate_X_y_pairs(train,optimum_lookback)\n",
    "test = generate_X_y_pairs(test,optimum_lookback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe500d98",
   "metadata": {},
   "source": [
    "##### Step 3b: Extract features about trend of time series\n",
    "In this step we define the rolling mean and rolling variance of a window. Since we are trying to forecast the last value of each window, only the first optimum_lookback values in the window are used to compute rolling features. The last value (that we are trying to forecast) is not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273707b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rolling mean information\n",
    "train['rolling_mean'] = 0\n",
    "test['rolling_mean'] = 0\n",
    "\n",
    "train['rolling_var'] = 0\n",
    "test['rolling_var'] = 0\n",
    "\n",
    "# calculate rolling mean of window\n",
    "train['rolling_mean'] = train.iloc[:,0:optimum_lookback].mean(axis=1)\n",
    "test['rolling_mean'] = test.iloc[:,0:optimum_lookback].mean(axis=1)\n",
    "\n",
    "# calculate rolling variance of window\n",
    "train['rolling_var'] = train.iloc[:, 0:optimum_lookback].var(axis=1)\n",
    "test['rolling_var'] = test.iloc[:, 0:optimum_lookback].var(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52625dd1",
   "metadata": {},
   "source": [
    "#####  Step 3c: Define labels and features\n",
    "- Features will help the XGboost algorithm to predict the output value, the label. We will use all the traffic flow information at previous timesteps (t-n,t-n+1...,t-2,t-1), as well as the rest of the features (time and trend information) as features.\n",
    "- Label will be out target value, the Traffic Flow at timestep t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "features = train.drop(['Time Column','Traffic Flow 1(t)','Traffic Flow','hour', 'hour_sin','day_of_week_cos', 'dayofweek', 'day', 'minute'],axis=1).columns\n",
    "\n",
    "# define labels\n",
    "labels = ['Traffic Flow 1(t)']\n",
    "\n",
    "# create X_train and y_train data sets\n",
    "X_train = train[features]\n",
    "y_train = train[labels]\n",
    "\n",
    "# create X_test and y_test data sets\n",
    "X_test = test[features]\n",
    "y_test = test[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape information about X and y training and testing sets\n",
    "print(\"Shape of X_train is: \",X_train.shape)\n",
    "print(\"Shape of y_train is: \",y_train.shape)\n",
    "print(\"Shape of X_test is: \",X_test.shape)\n",
    "print(\"Shape of y_test is: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96da71",
   "metadata": {},
   "source": [
    "##### Step 3d: Apply GridSearchCV\n",
    "Use a 5-fold K-fold cross-validation was used for learning the optimum hyperparameters for the XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318433fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperparameters using XGboost\n",
    "xgb_params = {\n",
    "    \"n_estimators\": [500],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"max_depth\": [3,9],\n",
    "    \"objective\":['reg:squarederror'],\n",
    "    \"gamma\": [0,1,2],\n",
    "    \"alpha\": [0,0.1,1]\n",
    "}\n",
    "\n",
    "# define GridSearchCV object\n",
    "xgb_grid = GridSearchCV(xgb.XGBRegressor(), \n",
    "                        xgb_params,cv=3,\n",
    "                        verbose=1,\n",
    "                        scoring= \"neg_mean_squared_error\")\n",
    "\n",
    "# train on train data using 5 fold CV\n",
    "xgb_grid.fit(X_train,y_train,\n",
    "             early_stopping_rounds=5,\n",
    "             eval_set=[(X_train, y_train), (X_test,y_test)],\n",
    "             eval_metric='rmse')\n",
    "\n",
    "# print the best hyperparameters and corresponding score\n",
    "print(\"Best Hyperparameters: \", xgb_grid.best_params_)\n",
    "print(\"Best Score: \", xgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f574e48",
   "metadata": {},
   "source": [
    "#### Step 4: Create dataframe to save actual values and model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predictions = pd.DataFrame(index=pd.to_datetime(test['Time Column']))\n",
    "total_predictions['Actual'] = y_test['Traffic Flow 1(t)'].values\n",
    "total_predictions['Path'] = X_test['Path'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ec4d5",
   "metadata": {},
   "source": [
    "#### Step 5: Train an XGboost model\n",
    "We will fit all the data in this model using the optimum parameters we found above. The optimum parameters for this dataset are: \n",
    "- n_estimators = 500\n",
    "- learning_rate = 0.1\n",
    "- max_depth = 9\n",
    "- alpha = 0.1\n",
    "- gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0afc734",
   "metadata": {},
   "source": [
    "Create XGBoost model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29cc36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGBoost regressor, use also gamma and alpha values for regularization\n",
    "reg = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=500,\n",
    "    early_stopping_rounds=5,\n",
    "    max_depth=9 ,\n",
    "    learning_rate=0.1,\n",
    "    gamma=1,\n",
    "    alpha=0.1    \n",
    ")\n",
    "\n",
    "# train the XGBoost model with feature weights, use validation set on test data)\n",
    "reg.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_test,y_test)], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2f63e",
   "metadata": {},
   "source": [
    "Feature Importance of XGBoost model\n",
    "Show how our model used the feautres, in order to perform splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d606b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how much each of the features was used in our regression analysis\n",
    "plt.figure(figsize=(15,5))\n",
    "fi = pd.DataFrame(data=reg.feature_importances_,index=reg.feature_names_in_,columns=['importance'])\n",
    "fi.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4acd6",
   "metadata": {},
   "source": [
    "Plot train and validation errors of XGBoost model in the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8bce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire the values of train and validation errors\n",
    "results = reg.evals_result()\n",
    "train_errors = results['validation_0']['rmse']\n",
    "validation_errors = results['validation_1']['rmse']\n",
    "\n",
    "# plot the validation and training errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_errors, label='Train Error (Train Set)')\n",
    "plt.plot(validation_errors, label='Validation Error (Test Set)')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('RMSE score of XGBoost')\n",
    "plt.title('XGBoost Model Training and Validation Errors')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a4060",
   "metadata": {},
   "source": [
    "Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26097705",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgboost = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de228b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Calculate the MAE of XGBoost model\n",
    "mae_xgboost = mean_absolute_error(y_test, y_pred_xgboost)\n",
    "print(\"Mae is: \",mae_xgboost)\n",
    "\n",
    "# Calculate RMSE score of XGBoost model\n",
    "rmse_xgboost = mean_squared_error(y_test, y_pred_xgboost, squared=False)\n",
    "print(\"RMSE score:\", rmse_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8c412",
   "metadata": {},
   "source": [
    "Show example of how well the model learned the trainning data.\n",
    "\n",
    "\n",
    "Use a random path to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the trainning set\n",
    "y_pred_train = reg.predict(X_train)\n",
    "\n",
    "# add information in the trainning set\n",
    "train_predictions = pd.DataFrame(index=pd.to_datetime(train['Time Column']))\n",
    "train_predictions.insert(0,'Train Actual',y_train['Traffic Flow 1(t)'].values)\n",
    "train_predictions.insert(1,'Train Predicted',y_pred_train)\n",
    "train_predictions.insert(2,'Path',X_train['Path'].values)\n",
    "\n",
    "# plot an example of the train-predictions at path 0 of the datase\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(train_predictions[train_predictions['Path']==0]['Train Actual'],\".-\",label='Actual Values')\n",
    "plt.plot(train_predictions[train_predictions['Path']==0]['Train Predicted'],\".-\",label='Predictions on train set using XGboost')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Traffic Flow in current path')\n",
    "plt.title('Forecast on train set using XGBoost model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047715b5",
   "metadata": {},
   "source": [
    "As we can see, the model has learned the trend and (almost) the seasonality of the data. \n",
    "\n",
    "Despite the non-linearity of Traffic Flow, XGBoost performs quite well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffadc53",
   "metadata": {},
   "source": [
    "Show an example of actual values vs predicted on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pd.DataFrame(index=pd.to_datetime(test['Time Column']))\n",
    "test_predictions.insert(0,'Test Actual',y_test['Traffic Flow 1(t)'].values)\n",
    "test_predictions.insert(1,'Test Predicted',y_pred_xgboost)\n",
    "test_predictions.insert(2,'Path',X_test['Path'].values)\n",
    "\n",
    "# plot an example of the train-predictions\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(test_predictions[test_predictions['Path']==0]['Test Actual'],\".-\",label='Actual Values')\n",
    "plt.plot(test_predictions[test_predictions['Path']==0]['Test Predicted'],\".-\",label='Predicted Values from XGBoost')\n",
    "plt.xlabel('Time information')\n",
    "plt.ylabel('Traffic Flow in current path')\n",
    "plt.title('Actual Values vs. Predicted on random Path of test set (XGBoost model)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285fb8e4",
   "metadata": {},
   "source": [
    "#### Step 6: Train LSTM for time series forecasting\n",
    "This is the second model that we will train on the same dataset. We use the optimum lookback score that we computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ca931",
   "metadata": {},
   "source": [
    "##### Step 6a: Create the train and test sets as before, then scale them\n",
    "Use the same train and test sets as in the XGBoost model. In order to create more accurate predictions, we scale those two sets before giving them as input to the LSTM RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_tr = scaler.fit_transform(X_train)\n",
    "X_te = scaler.fit_transform(X_test)\n",
    "\n",
    "y_tr = scaler.fit_transform(y_train)\n",
    "y_te = scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3D train set for LSTM\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "for i in range(len(X_tr)):\n",
    "    trainX.append(X_tr[i:i+1,:])\n",
    "    \n",
    "for i in range(len(y_tr)):\n",
    "    trainY.append(y_tr[i:i+1,:])\n",
    "    \n",
    "trainX, trainY = np.array(trainX), np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3D test set FOR LSTM\n",
    "testX = []\n",
    "testY = []\n",
    "\n",
    "for i in range(len(X_te)):\n",
    "    testX.append(X_te[i:i+1,:])\n",
    "    \n",
    "for i in range(len(y_te)):\n",
    "    testY.append(y_te[i:i+1,:])\n",
    "    \n",
    "testX, testY = np.array(testX), np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes of the created tests\n",
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb28de",
   "metadata": {},
   "source": [
    "##### Step 6b: Use the LSTM model for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680cd6b",
   "metadata": {},
   "source": [
    "Import libraries and define the LSTM structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58784c29",
   "metadata": {},
   "source": [
    "In the LSTM model, we include dense layers to capture more complex non linear patterns and dropout layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(25, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(12, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Additional Dense layers\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(3, activation='linear'))\n",
    "model.add(Dense(trainY.shape[1]))  # Output layer\n",
    "\n",
    "\n",
    "# Add early stopping mechanism\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa7c06",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcfc0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile LSTM model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d979a04",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on training data\n",
    "history = model.fit(trainX, trainY, \n",
    "          epochs=50, \n",
    "          batch_size=len(trainX)//100,\n",
    "          validation_data=(testX,testY), \n",
    "          verbose=1, \n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00843c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test data\n",
    "loss_lstm = model.evaluate(testX, testY)\n",
    "print(\"Test Loss:\", loss_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT TRAIN VS. VALIDATION LOSS OF LSTM MODEL ###\n",
    "\n",
    "# extract training and validation losses from the history object\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# plot the losses\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Training Loss vs Validation Loss of LSTM model')\n",
    "plt.legend()\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2651c4",
   "metadata": {},
   "source": [
    "Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "y_pred_lstm = model.predict(testX)\n",
    "y_pred_lstm = scaler.inverse_transform(y_pred_lstm.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c92fb",
   "metadata": {},
   "source": [
    "Evaluate the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the MAE score of LSTM model\n",
    "mae_lstm = mean_absolute_error(scaler.inverse_transform(testY.reshape(-1,1)),y_pred_lstm)\n",
    "print(\"Mae LSTM is: \",mae_lstm)\n",
    "\n",
    "# Calculate the RMSE score of LSTM model\n",
    "rmse_lstm = mean_squared_error(scaler.inverse_transform(testY.reshape(-1,1)), y_pred_lstm, squared=False)\n",
    "print(\"RMSE LSTM score:\", rmse_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b77a5",
   "metadata": {},
   "source": [
    "Show example of how well the LSTM captured/learned the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the trainning set\n",
    "y_pred_train_LSTM = model.predict(trainX)\n",
    "\n",
    "# add information in the trainning set\n",
    "train_predictions = pd.DataFrame(index=pd.to_datetime(train['Time Column']))\n",
    "train_predictions.insert(0,'Train Actual',y_train['Traffic Flow 1(t)'].values)\n",
    "train_predictions.insert(1,'Train Predicted',scaler.inverse_transform(y_pred_train_LSTM.reshape(-1,1)))\n",
    "train_predictions.insert(2,'Path',X_train['Path'].values)\n",
    "\n",
    "# plot an example of the train-predictions\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(train_predictions[train_predictions['Path']==0][['Train Actual','Train Predicted']],\".-\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Traffic Flow in current path')\n",
    "plt.title('Forecast on train set using LSTM model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5f2dc",
   "metadata": {},
   "source": [
    "#### Step 7: Create Random Forest model\n",
    "This is the third model that we train on the same data, in order to forecast the traffic flow prediction. This algorithm is quite the same with the XGBoost, because both of them use decision trees to make forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbfd633",
   "metadata": {},
   "source": [
    "##### Step 7a: Find optimum parameters for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'max_depth': [5, 7],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'n_estimators': [50]\n",
    "}\n",
    "\n",
    "# create the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "# create the GridSearchCV object\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=3, scoring='neg_mean_squared_error',verbose=1)\n",
    "\n",
    "# fit the GridSearchCV object on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# print the best parameters and the best score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4745a",
   "metadata": {},
   "source": [
    "##### Step 7b: Create and train best random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model after Grid Search\n",
    "best_model = RandomForestRegressor(n_estimators=150,\n",
    "                                  min_samples_split=10,\n",
    "                                  max_depth=7,\n",
    "                                  verbose=1,\n",
    "                                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db6c01",
   "metadata": {},
   "source": [
    "##### Step 7c: Make predictions on train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ab654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "y_pred_rf = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the trainning set using random forest\n",
    "y_pred_train_rf = best_model.predict(X_train)\n",
    "\n",
    "# add information in the trainning set\n",
    "train_predictions = pd.DataFrame(index=pd.to_datetime(train['Time Column']))\n",
    "train_predictions.insert(0,'Train Actual',y_train['Traffic Flow 1(t)'].values)\n",
    "train_predictions.insert(1,'Train Predicted',y_pred_train_rf)\n",
    "train_predictions.insert(2,'Path',X_train['Path'].values)\n",
    "\n",
    "# plot an example of the train-predictions\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(train_predictions[train_predictions['Path']==0][['Train Actual','Train Predicted']],\".-\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Traffic Flow in current path')\n",
    "plt.title('Forecast on train set using random forest model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682889e",
   "metadata": {},
   "source": [
    "##### Step 7d: Evalueate the random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b808b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the MAE\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "print(\"MAE: \", mae_rf)\n",
    "\n",
    "# calculate the RMSE\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "print(\"RMSE: \", rmse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc06d88",
   "metadata": {},
   "source": [
    "#### Step 8: Train an Encoder-Decoder model\n",
    "This is the fourth model that we are using, in order to forecast the traffic flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0362f5",
   "metadata": {},
   "source": [
    "##### Step 8a: Initialize and compile the Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08038bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add necessary imports\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train and test set to infinite datasets for faster training\n",
    "BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((testX, testY))\n",
    "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea133eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE THE MODEL ###\n",
    "\n",
    "# Encoder part\n",
    "enco_deco = Sequential()\n",
    "enco_deco.add(LSTM(100, input_shape=(1, trainX.shape[2]), return_sequences=True))\n",
    "enco_deco.add(Dropout(0.2))  # Add dropout to the first LSTM layer\n",
    "enco_deco.add(LSTM(units=50, return_sequences=True))\n",
    "enco_deco.add(Dropout(0.2))  # Add dropout to the second LSTM layer\n",
    "enco_deco.add(LSTM(units=15))  \n",
    "\n",
    "# Feature vector\n",
    "enco_deco.add(layers.RepeatVector(1))  \n",
    "\n",
    "# Decoder part\n",
    "enco_deco.add(LSTM(units=100, return_sequences=True))\n",
    "enco_deco.add(LSTM(units=50, return_sequences=True))\n",
    "enco_deco.add(TimeDistributed(layers.Dense(units=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9960f03",
   "metadata": {},
   "source": [
    "Similart to the LSTM model, we include dense layers to capture more complex non linear patterns and dropout layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enco_deco.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43bf312",
   "metadata": {},
   "source": [
    "##### Step 8b: Train the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c15850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the enco-deco model\n",
    "history = enco_deco.fit(train_univariate, \n",
    "                        epochs=50, \n",
    "                        steps_per_epoch=len(trainX)//100,\n",
    "                        validation_data=val_univariate, \n",
    "                        validation_steps=len(testX)//100, \n",
    "                        verbose=1, \n",
    "                        callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using Encoder-Decoder model\n",
    "predicted_enco_deco = enco_deco.predict(testX)\n",
    "predicted_enco_deco = scaler.inverse_transform(predicted_enco_deco.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e392f41",
   "metadata": {},
   "source": [
    "##### Step 8c: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d664822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE of Encoder - Decoder model\n",
    "mae_enco_deco = mean_absolute_error(y_test, predicted_enco_deco)\n",
    "\n",
    "# Calculate RMSE Encoder - Decoder model\n",
    "rmse_enco_deco = mean_squared_error(y_test, predicted_enco_deco, squared=False)\n",
    "\n",
    "print(\"MAE:\", mae_enco_deco)\n",
    "print(\"RMSE:\", rmse_enco_deco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94afd443",
   "metadata": {},
   "source": [
    "Plot train loss vs. validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loss and validation loss values and pllot them in the same graph\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the loss and validation loss\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss of Encoder-Decoder model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73570ef3",
   "metadata": {},
   "source": [
    "Forecast on train set using Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ad730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the trainning set\n",
    "y_pred_train_enco_deco = enco_deco.predict(trainX)\n",
    "y_pred_train_enco_deco = scaler.inverse_transform(y_pred_train_enco_deco.reshape(-1, 1))\n",
    "\n",
    "# add information in the trainning set\n",
    "train_predictions = pd.DataFrame(index=pd.to_datetime(train['Time Column']))\n",
    "train_predictions.insert(0,'Train Actual',y_train['Traffic Flow 1(t)'].values)\n",
    "train_predictions.insert(1,'Train Predicted',y_pred_train_enco_deco)\n",
    "train_predictions.insert(2,'Path',X_train['Path'].values)\n",
    "\n",
    "# plot an example of the train-predictions\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(train_predictions[train_predictions['Path']==0][['Train Actual','Train Predicted']],\".-\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Traffic Flow in current path')\n",
    "plt.title('Forecast on train set using Encoder-Decoder model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e8f48",
   "metadata": {},
   "source": [
    "#### Step 9: Add predictions of the four models to the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4150e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predictions of XGBoost model to a dataframe\n",
    "total_predictions['Predicted XGBoost'] = y_pred_xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predictions of LSTM model to a dataframe\n",
    "total_predictions['Predicted LSTM'] = scaler.inverse_transform(y_pred_lstm.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add predictions of random forest\n",
    "total_predictions['Predicted Random Forest'] = y_pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add predictions of encoder decoder model\n",
    "total_predictions['Predicted Encoder Decoder Model'] = predicted_enco_deco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataframe with predictions\n",
    "total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3fbae",
   "metadata": {},
   "source": [
    "#### Step 10: Add in the same dataframe the scores of each model, in order to compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with scores\n",
    "scores = pd.DataFrame(columns=['Model','RMSE Score','MAE Score'])\n",
    "\n",
    "# define rows for dataframe\n",
    "new_row = {'Model': 'XGBoost', 'RMSE Score': rmse_xgboost, 'MAE Score': mae_xgboost}\n",
    "new_row1 = {'Model': 'LSTM', 'RMSE Score': rmse_lstm, 'MAE Score': mae_lstm}\n",
    "new_row2 = {'Model': 'Random Forest', 'RMSE Score': rmse_rf, 'MAE Score': mae_rf}\n",
    "new_row3 = {'Model': 'Encoder Decoder', 'RMSE Score': rmse_enco_deco, 'MAE Score': mae_enco_deco}\n",
    "\n",
    "# add rows to dataframe\n",
    "scores = scores.append(new_row, ignore_index=True)\n",
    "scores = scores.append(new_row1, ignore_index=True)\n",
    "scores = scores.append(new_row2, ignore_index=True)\n",
    "scores = scores.append(new_row3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283de723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataframe with scores\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bafcd",
   "metadata": {},
   "source": [
    "#### Step 11: Plot examples of actual vs Predicted for each of the models\n",
    "In this part, we select a random path of the dataset and compare (for each model) the actual Traffic Flow values vs. the predicted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results from XGBoost\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(total_predictions[total_predictions['Path']==0]['Actual'],label='Actuals')\n",
    "plt.plot(total_predictions[total_predictions['Path']==0]['Predicted XGBoost'],label='Predicted')\n",
    "plt.xlabel(\"Time information\")\n",
    "plt.ylabel('Traffic Flow values')\n",
    "plt.title('Actual vs Predicted traffic flow values of path 0 using XGBoost model')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02078a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results from LSTM\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(total_predictions[total_predictions['Path']==0]['Actual'],label='Actuals')\n",
    "plt.plot(total_predictions[total_predictions['Path']==0]['Predicted LSTM'],label='Predicted')\n",
    "plt.xlabel(\"Time information\")\n",
    "plt.ylabel('Traffic Flow values')\n",
    "plt.title('Actual vs Predicted traffic flow values of path 0 using LSTM model')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results from Random Forest\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(total_predictions[total_predictions['Path']==0]['Actual'],label='Actuals')\n",
    "plt.plot(total_predictions[total_predictions['Path']==0]['Predicted Random Forest'],label='Predicted')\n",
    "plt.xlabel(\"Time information\")\n",
    "plt.ylabel('Traffic Flow values')\n",
    "plt.title('Actual vs Predicted traffic flow values of path 0 using Random Forest model')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results from Encoder Decoder\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(total_predictions[total_predictions['Path']==0]['Actual'],label='Actuals')\n",
    "plt.plot(total_predictions[total_predictions['Path']==0]['Predicted Encoder Decoder Model'],label='Predicted')\n",
    "plt.xlabel(\"Time information\")\n",
    "plt.ylabel('Traffic Flow values')\n",
    "plt.title('Actual vs Predicted traffic flow values of path 0 using Encoder-Decoder model model')\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c41626",
   "metadata": {},
   "source": [
    "#### Step 12: Make visualizations on actual vs predicted traffic flow data\n",
    "Use best model's predictions from total_predictions dataframe and plot the residuals between the actual traffic flow values and predicted. \n",
    "\n",
    "The graph to be created will contain the sum of traffic flow per timestamp, as done before in the visualization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f00a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupd dataframe by timestamp\n",
    "grouped = total_predictions.groupby(total_predictions.index).sum()\n",
    "\n",
    "# add results to dataframe\n",
    "grouped = pd.DataFrame(grouped,index=grouped.index)\n",
    "\n",
    "# drop path column, as it is useless\n",
    "grouped.drop('Path', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba886d",
   "metadata": {},
   "source": [
    "##### Decide which model's predicted values will be used, based on the best RMSE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddf0c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# find the model with the best RMSE score (the lowest)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m index \u001b[38;5;241m=\u001b[39m scores[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE Score\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39midxmin()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "# find the model with the best RMSE score (the lowest)\n",
    "index = scores[['RMSE Score']].idxmin() # find the model with the minimun RMSE score (best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (scores.iloc[index]['Model'][0] == 'XGBoost'):\n",
    "    # create a plot to view the results\n",
    "    plt.figure(figsize=(15,5))\n",
    "    sns.lineplot(grouped,x=grouped.index,y='Actual',marker='o',linewidth=2.5,label='Actual')\n",
    "    sns.lineplot(grouped,x=grouped.index,y='Predicted XGBoost',marker='o',linewidth=2.5, color='orange',label='XGboost')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Sum of Traffic Flow')\n",
    "    plt.title('Sum of Traffic Flow in Every Path of the Dataset Over Time vs Predicted')\n",
    "    plt.show()\n",
    "\n",
    "elif (scores.iloc[index]['Model'][0] == 'LSTM'):\n",
    "      # create a plot to view the results\n",
    "      plt.figure(figsize=(15,5))\n",
    "      sns.lineplot(grouped,x=grouped.index,y='Actual',marker='o',linewidth=2.5,label='Actual')\n",
    "      sns.lineplot(grouped,x=grouped.index,y='Predicted XGBoost',marker='o',linewidth=2.5, color='orange',label='XGboost')\n",
    "      plt.xlabel('Time')\n",
    "      plt.ylabel('Sum of Traffic Flow')\n",
    "      plt.title('Sum of Traffic Flow in Every Path of the Dataset Over Time vs Predicted')\n",
    "      plt.show()\n",
    "\n",
    "elif (scores.iloc[index]['Model'][0] == 'Random Forest'):\n",
    "    # create a plot to view the results\n",
    "      plt.figure(figsize=(15,5))\n",
    "      sns.lineplot(grouped,x=grouped.index,y='Actual',marker='o',linewidth=2.5,label='Actual')\n",
    "      sns.lineplot(grouped,x=grouped.index,y='Predicted XGBoost',marker='o',linewidth=2.5, color='orange',label='XGboost')\n",
    "      plt.xlabel('Time')\n",
    "      plt.ylabel('Sum of Traffic Flow')\n",
    "      plt.title('Sum of Traffic Flow in Every Path of the Dataset Over Time vs Predicted')\n",
    "      plt.show()\n",
    "\n",
    "elif (scores.iloc[index]['Model'][0] == 'Encoder Decoder'):\n",
    "    # create a plot to view the results\n",
    "      plt.figure(figsize=(15,5))\n",
    "      sns.lineplot(grouped,x=grouped.index,y='Actual',marker='o',linewidth=2.5,label='Actual')\n",
    "      sns.lineplot(grouped,x=grouped.index,y='Predicted XGBoost',marker='o',linewidth=2.5, color='orange',label='XGboost')\n",
    "      plt.xlabel('Time')\n",
    "      plt.ylabel('Sum of Traffic Flow')\n",
    "      plt.title('Sum of Traffic Flow in Every Path of the Dataset Over Time vs Predicted')\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23ad7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalytics",
   "language": "python",
   "name": "dataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
